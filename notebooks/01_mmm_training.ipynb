{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "mmm_overview_header"
   },
   "source": [
    "# Advanced Marketing Mix Modeling (MMM) Pipeline\n",
    "## Global B2B Multi-Regional, Multi-Product Implementation\n",
    "\n",
    "### What This Notebook Does\n",
    "\n",
    "**Marketing Mix Modeling (MMM)** answers: *\"For every dollar we spend on LinkedIn vs. Google vs. Display, how much revenue do we get back?\"*\n",
    "\n",
    "Unlike digital attribution (last-click), MMM uses **statistical regression** to estimate causal effects of media spend on revenue, accounting for:\n",
    "- **Time delays** (spend today → revenue in 6-9 months for B2B)\n",
    "- **Diminishing returns** (doubling spend doesn't double results)\n",
    "- **External factors** (seasonality, economic conditions)\n",
    "\n",
    "### Core Techniques (cells 4-7)\n",
    "\n",
    "| Technique | What It Does | Why We Need It |\n",
    "|-----------|--------------|----------------|\n",
    "| **Geometric Adstock** | Spreads each week's spend across future weeks with exponential decay | LinkedIn ads today still influence buyers 4 weeks from now |\n",
    "| **Hill Saturation** | S-curve transformation that flattens at high spend | The 10th impression to the same person has ~0 value |\n",
    "| **Nevergrad Optimization** | Evolutionary algorithm to find best decay/saturation params | Can't grid-search 60 parameters; no gradients available |\n",
    "| **Ridge Regression** | L2-penalized linear model with positive constraints | Prevents overfitting; ensures \"more spend ≠ less revenue\" |\n",
    "\n",
    "### Validation & Uncertainty (cells 6, 8-9)\n",
    "\n",
    "| Feature | Purpose |\n",
    "|---------|---------|\n",
    "| **Time-Series CV** | Tests if model can predict FUTURE quarters (not just fit history) |\n",
    "| **Bootstrap CI** | Shows \"LinkedIn ROI is 3.2x [2.8-3.6]\" not just \"3.2x\" |\n",
    "| **MAPE Metric** | \"We're typically 12% off\" is more intuitive than R² |\n",
    "\n",
    "### Output (cells 12-13)\n",
    "\n",
    "Results are saved to `ATOMIC.MMM_MODEL_RESULT` with ROI, confidence intervals, marginal ROI, and recommended spend per Channel × Region × Product.\n",
    "\n",
    "---\n",
    "**Data Sources**\n",
    "- **Input**: `DIMENSIONAL.V_MMM_INPUT_WEEKLY` (weekly spend + revenue + controls)\n",
    "- **Output**: `ATOMIC.MMM_MODEL_RESULT` (joins to dimension tables via FK)\n",
    "\n",
    "**Infrastructure**: Snowflake Notebooks or SPCS | Python 3.11 | ~5-10 min runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "install_packages"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 0: Install Required Packages\n",
    "# =============================================================================\n",
    "# Note: nevergrad is not pre-installed in Snowflake notebooks\n",
    "# This cell requires EXTERNAL_ACCESS_INTEGRATIONS to be configured\n",
    "# Using os.system() for compatibility with headless SPCS execution\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "packages = [\"nevergrad\"]\n",
    "for pkg in packages:\n",
    "    os.system(f\"{sys.executable} -m pip install {pkg} -q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "imports_and_config"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Imports and Configuration\n",
    "# =============================================================================\n",
    "# \n",
    "# KEY LIBRARIES:\n",
    "# - nevergrad: Meta's derivative-free optimization library. We use it because\n",
    "#   MMM hyperparameters (adstock decay, saturation) don't have clean gradients.\n",
    "#   TwoPointsDE is an evolutionary algorithm that works well for ~10-100 params.\n",
    "#\n",
    "# - Ridge regression: Linear model with L2 penalty. We use Ridge (not OLS) because:\n",
    "#   (1) Marketing data is often collinear (channels spike together in Q4)\n",
    "#   (2) L2 shrinks coefficients toward zero, preventing wild estimates\n",
    "#   (3) We can't use Lasso (L1) because it zeros out channels entirely\n",
    "#\n",
    "# - scipy.optimize: For budget reallocation with business constraints (±30% limits)\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# LIBRARY CONTEXT FOR MAINTAINERS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# pandas/numpy: Standard data manipulation. No special versions required.\n",
    "#\n",
    "# sklearn.linear_model.Ridge:\n",
    "#   - Implements: β = (X'X + λI)⁻¹X'y (closed-form, fast)\n",
    "#   - alpha parameter = λ in the penalty term\n",
    "#   - We use default alpha=1.0 which is mild regularization\n",
    "#\n",
    "# sklearn.preprocessing.StandardScaler:\n",
    "#   - Transforms X to zero mean, unit variance: X_scaled = (X - μ) / σ\n",
    "#   - Critical for Ridge: penalty treats all features equally, so they must\n",
    "#     be on the same scale. Without scaling, a $1M spend column would have\n",
    "#     tiny coefficients vs. a [0,1] saturation column with huge coefficients.\n",
    "#\n",
    "# nevergrad:\n",
    "#   - Meta's library for \"black-box\" optimization (no gradients needed)\n",
    "#   - TwoPointsDE: Differential Evolution variant, population-based\n",
    "#   - \"budget\" = number of function evaluations (not $$ budget!)\n",
    "#   - See: https://facebookresearch.github.io/nevergrad/\n",
    "#\n",
    "# scipy.optimize.minimize:\n",
    "#   - General-purpose minimizer with many algorithms\n",
    "#   - method='SLSQP': Sequential Least Squares Programming\n",
    "#     Handles equality + inequality constraints efficiently\n",
    "#   - We use it for budget allocation where SLSQP's constraint handling shines\n",
    "#\n",
    "# ALTERNATIVES CONSIDERED (not used):\n",
    "# - PyMC/Stan: Bayesian MMM (e.g., Robyn). More principled uncertainty but\n",
    "#   10x slower and requires MCMC tuning expertise.\n",
    "# - LightweightMMM (Google): Similar approach but JAX-based. We chose sklearn\n",
    "#   for broader compatibility with Snowflake's Python environment.\n",
    "# - CausalImpact: Good for single interventions, not continuous spend allocation.\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# Snowflake\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "# ML/Stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import nevergrad as ng\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)  # Reproducibility for bootstrap sampling\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class MMMConfig:\n",
    "    \"\"\"\n",
    "    Configuration for MMM model training.\n",
    "    \n",
    "    GRANULARITY CHOICES:\n",
    "    - geo_level: Controls geographic aggregation. Options:\n",
    "        - \"GLOBAL\": Aggregate all regions (best for sparse data, ~10 channel groups)\n",
    "        - \"SUPER_REGION\": 3-4 regions per channel (needs 50+ weeks per combo)\n",
    "        - \"REGION\" or \"COUNTRY\": More granular (needs very rich data)\n",
    "      Rule of thumb: need ~50+ weeks of non-zero REVENUE per Channel×Geo combo.\n",
    "    \n",
    "    - product_level: SEGMENT (4 groups) vs CATEGORY (23 groups). More granular = \n",
    "      more actionable but requires more data. Start with SEGMENT, drill down if R² holds.\n",
    "    \n",
    "    HYPERPARAMETER SEARCH:\n",
    "    - nevergrad_budget: 500 iterations is a good balance. Robyn uses 2000+ but we're\n",
    "      optimizing fewer params (no decomposition). Increase if CV MAPE is unstable.\n",
    "    \n",
    "    VALIDATION:\n",
    "    - cv_train_weeks=52: Full year captures seasonality (Q1 budget flush, Q4 holidays)\n",
    "    - cv_test_weeks=13: Quarter-out holdout mimics real forecasting use case\n",
    "    \n",
    "    DATA SPARSITY NOTE:\n",
    "    If CV MAPE > 50%, the data is likely too sparse for the chosen granularity.\n",
    "    Switch geo_level to \"GLOBAL\" to aggregate across regions and improve model stability.\n",
    "    \"\"\"\n",
    "    # Data sources\n",
    "    input_view: str = \"DIMENSIONAL.V_MMM_INPUT_WEEKLY\"\n",
    "    output_table: str = \"ATOMIC.MMM_MODEL_RESULT\"\n",
    "    \n",
    "    # Model granularity - use GLOBAL for channel-only modeling (most robust)\n",
    "    # Use SUPER_REGION only if you have 50+ weeks with revenue per channel-region\n",
    "    geo_level: str = \"GLOBAL\"         # GLOBAL (recommended), SUPER_REGION, REGION, or COUNTRY\n",
    "    product_level: str = \"SEGMENT\"    # SEGMENT, DIVISION, or CATEGORY\n",
    "    \n",
    "    # Hyperparameter optimization\n",
    "    nevergrad_budget: int = 500       # Evolutionary algorithm iterations\n",
    "    ridge_alpha: float = 10.0         # L2 penalty strength (10.0 for sparse data, 1.0 for rich data)\n",
    "    \n",
    "    # Time-series cross-validation (rolling window, never peek at future)\n",
    "    cv_train_weeks: int = 52          # 1 year training window\n",
    "    cv_test_weeks: int = 13           # 1 quarter holdout (13 weeks)\n",
    "    cv_step_weeks: int = 13           # Roll forward 1 quarter between folds\n",
    "    \n",
    "    # Bootstrap for uncertainty quantification\n",
    "    n_bootstrap: int = 100            # Resample iterations (100 is standard)\n",
    "    confidence_level: float = 0.90    # 90% CI = 5th to 95th percentile\n",
    "    \n",
    "    # Budget optimizer constraints\n",
    "    budget_change_limit: float = 0.30 # ±30% per channel (realistic for CMO approval)\n",
    "    \n",
    "    # Model versioning (for tracking in MMM_MODEL_RESULT table)\n",
    "    model_version: str = \"v3.1_channel_only\"\n",
    "\n",
    "config = MMMConfig()\n",
    "print(f\"MMM Configuration initialized: {config.model_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "load_data_from_snowflake"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Connect to Snowflake and Load Data\n",
    "# =============================================================================\n",
    "\n",
    "session = get_active_session()\n",
    "print(f\"Connected to Snowflake: {session.get_current_database()}.{session.get_current_schema()}\")\n",
    "\n",
    "# Load weekly aggregated data from dimensional view\n",
    "df_raw = session.table(config.input_view).to_pandas()\n",
    "\n",
    "# Standardize column names to uppercase\n",
    "df_raw.columns = df_raw.columns.str.upper()\n",
    "\n",
    "# Map view column names to expected model column names\n",
    "# The view uses _NAME/_CODE suffixes, but the model expects simple names\n",
    "column_mapping = {\n",
    "    'SUPER_REGION_NAME': 'SUPER_REGION',\n",
    "    'REGION_NAME': 'REGION',\n",
    "    'COUNTRY_NAME': 'COUNTRY',\n",
    "    'SEGMENT_NAME': 'SEGMENT',\n",
    "    'DIVISION_NAME': 'DIVISION',\n",
    "    'CATEGORY_NAME': 'CATEGORY',\n",
    "    'CHANNEL_CODE': 'CHANNEL',\n",
    "    'AVG_PMI': 'PMI_INDEX',\n",
    "    'AVG_COMPETITOR_SOV': 'COMPETITOR_SOV',\n",
    "    'AVG_INDUSTRY_GROWTH': 'INDUSTRY_GROWTH'\n",
    "}\n",
    "df_raw = df_raw.rename(columns=column_mapping)\n",
    "\n",
    "print(f\"\\nLoaded {len(df_raw):,} rows from {config.input_view}\")\n",
    "print(f\"Date range: {df_raw['WEEK_START'].min()} to {df_raw['WEEK_START'].max()}\")\n",
    "print(f\"\\nColumns: {df_raw.columns.tolist()}\")\n",
    "\n",
    "# Check dimension coverage\n",
    "print(f\"\\nDimension coverage:\")\n",
    "print(f\"  SUPER_REGION: {df_raw['SUPER_REGION'].dropna().unique().tolist()}\")\n",
    "print(f\"  CHANNEL: {df_raw['CHANNEL'].dropna().unique().tolist()}\")\n",
    "segment_vals = df_raw['SEGMENT'].dropna().unique().tolist() if 'SEGMENT' in df_raw.columns and df_raw['SEGMENT'].notna().any() else []\n",
    "print(f\"  SEGMENT: {segment_vals if segment_vals else 'All NULL - will use ALL'}\")\n",
    "\n",
    "# Preview data\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "prepare_mmm_data_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Data Preparation and Feature Engineering\n",
    "# =============================================================================\n",
    "#\n",
    "# WHY THESE FEATURES MATTER:\n",
    "#\n",
    "# 1. COMPOSITE KEYS (Channel_Region_Product):\n",
    "#    We model each combination separately because \"LinkedIn in EMEA for Safety\"\n",
    "#    behaves differently than \"LinkedIn in APAC for Healthcare\". This is the core\n",
    "#    value prop: granular ROI, not just \"LinkedIn overall\".\n",
    "#\n",
    "# 2. FOURIER TERMS FOR SEASONALITY:\n",
    "#    Instead of 52 dummy variables (one per week), we use sin/cos waves.\n",
    "#    - SIN_1/COS_1: Annual cycle (captures \"Q4 always high\")\n",
    "#    - SIN_2/COS_2: Semi-annual (captures \"Q2 and Q4 different from Q1/Q3\")\n",
    "#    - SIN_3/COS_3: Quarterly patterns\n",
    "#    Benefit: 6 features instead of 52, prevents overfitting, captures smooth patterns.\n",
    "#\n",
    "# 3. TREND COMPONENT:\n",
    "#    Linear time trend captures \"revenue grows 5% per year regardless of marketing\".\n",
    "#    Without this, model would attribute organic growth to whichever channel scaled up.\n",
    "#\n",
    "# 4. Q1/Q3 FLAGS:\n",
    "#    B2B-specific: Q1 = budget flush, Q3 = pre-year-end push. Binary flags let the\n",
    "#    model learn \"Q1 has 20% higher baseline\" without needing exact week-of-year.\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEEPER DIVE: FOURIER SEASONALITY (From Signal Processing)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# Any periodic signal can be decomposed into a sum of sinusoids (Fourier theorem).\n",
    "# For a signal with period T (52 weeks for annual seasonality):\n",
    "#\n",
    "#   f(t) = a₀ + Σₖ [aₖ·cos(2πkt/T) + bₖ·sin(2πkt/T)]\n",
    "#\n",
    "# Each k represents a \"harmonic\" of the fundamental frequency:\n",
    "#   k=1: Fundamental (one complete cycle per year)\n",
    "#   k=2: First harmonic (two cycles per year, i.e., semi-annual)\n",
    "#   k=3: Second harmonic (four cycles, i.e., quarterly)\n",
    "#\n",
    "# WHY SIN AND COS PAIRS:\n",
    "# A single sinusoid aₖ·cos(2πkt/T + φ) has both amplitude aₖ and phase φ.\n",
    "# Using both sin AND cos with separate coefficients, the regression can learn\n",
    "# any amplitude and phase:\n",
    "#   aₖ·cos(2πkt/T + φ) = (aₖ·cos(φ))·cos(2πkt/T) + (aₖ·sin(φ))·sin(2πkt/T)\n",
    "#                       = β_cos·cos(2πkt/T) + β_sin·sin(2πkt/T)\n",
    "#\n",
    "# So the model learns β_cos and β_sin, and we get:\n",
    "#   Amplitude = √(β_cos² + β_sin²)\n",
    "#   Phase = arctan(β_sin / β_cos)\n",
    "#\n",
    "# WHY 3 HARMONICS:\n",
    "# - k=1,2,3 capture annual, semi-annual, and quarterly patterns\n",
    "# - Higher harmonics (k=4,5,...) would capture weekly fluctuations\n",
    "# - More harmonics = more flexible but higher overfitting risk\n",
    "# - Rule of thumb: use k = 1 to (n/2-1) where n = periods per year ÷ 10\n",
    "#   For weekly data with 52 periods/year: up to k ≈ 3 is reasonable\n",
    "#\n",
    "# ALTERNATIVE: SEASONAL DUMMIES\n",
    "# 52 binary indicators (one per week) is equivalent to unlimited harmonics.\n",
    "# Downsides: 52 extra parameters, can't extrapolate, captures noise not signal.\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_mmm_data(df: pd.DataFrame, config: MMMConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare data for MMM modeling with proper granularity and features.\n",
    "    \n",
    "    Features added:\n",
    "    - Composite keys for channel × region × product (granular attribution)\n",
    "    - Fourier seasonality (smooth annual/semi-annual patterns, 6 features vs 52 dummies)\n",
    "    - Linear trend (isolate organic growth from marketing impact)\n",
    "    - B2B fiscal flags (Q1 budget flush, Q3 push)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure datetime\n",
    "    df['WEEK_START'] = pd.to_datetime(df['WEEK_START'])\n",
    "    \n",
    "    # Fill missing values\n",
    "    numeric_cols = ['SPEND', 'IMPRESSIONS', 'CLICKS', 'REVENUE', 'PMI_INDEX', 'COMPETITOR_SOV']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Create composite dimension keys based on config granularity\n",
    "    geo_col = config.geo_level\n",
    "    prod_col = config.product_level\n",
    "    \n",
    "    # Handle GLOBAL geo_level - aggregate all regions into single \"GLOBAL\" value\n",
    "    # This is recommended for sparse data to ensure sufficient sample size per channel\n",
    "    if geo_col == \"GLOBAL\":\n",
    "        print(f\"  Using GLOBAL geo aggregation (channel-only modeling)\")\n",
    "        df['GEO_KEY'] = 'GLOBAL'\n",
    "        geo_col = 'GEO_KEY'\n",
    "    else:\n",
    "        print(f\"  Looking for geo_col='{geo_col}' in columns: {'YES' if geo_col in df.columns else 'NO'}\")\n",
    "        if geo_col in df.columns:\n",
    "            df[geo_col] = df[geo_col].fillna('UNKNOWN')\n",
    "        else:\n",
    "            df[geo_col] = 'ALL'\n",
    "            print(f\"  WARNING: {geo_col} column not found, using 'ALL'\")\n",
    "    \n",
    "    # Debug: print what columns we have\n",
    "    print(f\"  Looking for prod_col='{prod_col}' in columns: {'YES' if prod_col in df.columns else 'NO'}\")\n",
    "    print(f\"  Looking for 'CHANNEL' in columns: {'YES' if 'CHANNEL' in df.columns else 'NO'}\")\n",
    "        \n",
    "    if prod_col in df.columns:\n",
    "        df[prod_col] = df[prod_col].fillna('ALL')  # Use 'ALL' for null product since we don't have segment data\n",
    "    else:\n",
    "        df[prod_col] = 'ALL'\n",
    "        print(f\"  WARNING: {prod_col} column not found, using 'ALL'\")\n",
    "        \n",
    "    if 'CHANNEL' in df.columns:\n",
    "        df['CHANNEL'] = df['CHANNEL'].fillna('UNKNOWN')\n",
    "    else:\n",
    "        df['CHANNEL'] = 'UNKNOWN'\n",
    "        print(\"  WARNING: CHANNEL column not found!\")\n",
    "    \n",
    "    # Composite key: Channel_Region_Product\n",
    "    # With GLOBAL geo_level, this becomes Channel_GLOBAL_ALL (effectively channel-only)\n",
    "    df['CHANNEL_KEY'] = (\n",
    "        df['CHANNEL'].astype(str) + '_' + \n",
    "        df[geo_col].astype(str) + '_' + \n",
    "        df[prod_col].astype(str)\n",
    "    )\n",
    "    \n",
    "    # Add time features for seasonality\n",
    "    df['WEEK_OF_YEAR'] = df['WEEK_START'].dt.isocalendar().week.astype(int)\n",
    "    df['YEAR'] = df['WEEK_START'].dt.year\n",
    "    df['TREND'] = (df['WEEK_START'] - df['WEEK_START'].min()).dt.days / 365.25\n",
    "    \n",
    "    # Fourier terms for seasonality (annual cycle)\n",
    "    for k in [1, 2, 3]:\n",
    "        df[f'SIN_{k}'] = np.sin(2 * np.pi * k * df['WEEK_OF_YEAR'] / 52)\n",
    "        df[f'COS_{k}'] = np.cos(2 * np.pi * k * df['WEEK_OF_YEAR'] / 52)\n",
    "    \n",
    "    # Q1/Q3 seasonality flag (B2B budget cycles)\n",
    "    df['Q1_FLAG'] = ((df['WEEK_START'].dt.month >= 1) & (df['WEEK_START'].dt.month <= 3)).astype(int)\n",
    "    df['Q3_FLAG'] = ((df['WEEK_START'].dt.month >= 7) & (df['WEEK_START'].dt.month <= 9)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Prepare data\n",
    "df = prepare_mmm_data(df_raw, config)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nUnique channel keys: {df['CHANNEL_KEY'].nunique()}\")\n",
    "print(f\"Unique {config.geo_level}: {df[config.geo_level].nunique() if config.geo_level in df.columns else 'N/A'}\")\n",
    "print(f\"Unique {config.product_level}: {df[config.product_level].nunique() if config.product_level in df.columns else 'N/A'}\")\n",
    "print(f\"\\nSample channel keys: {df['CHANNEL_KEY'].unique()[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "adstock_saturation_functions_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Adstock and Saturation Transformation Functions\n",
    "# =============================================================================\n",
    "#\n",
    "# THESE ARE THE TWO CORE CONCEPTS IN MODERN MMM:\n",
    "#\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "# │ 1. ADSTOCK (Carryover Effect)                                               │\n",
    "# │                                                                             │\n",
    "# │ PROBLEM: A $100k LinkedIn campaign on Week 1 doesn't just affect Week 1.   │\n",
    "# │ In B2B, someone sees an ad, researches for 3 weeks, then converts.         │\n",
    "# │                                                                             │\n",
    "# │ SOLUTION: \"Spread\" the spend across future weeks with exponential decay:   │\n",
    "# │                                                                             │\n",
    "# │   Week 1: $100k    →  Effective: $100k                                     │\n",
    "# │   Week 2: $0       →  Effective: $70k  (70% of previous)                   │\n",
    "# │   Week 3: $0       →  Effective: $49k  (70% of $70k)                       │\n",
    "# │   Week 4: $0       →  Effective: $34k  ...and so on                        │\n",
    "# │                                                                             │\n",
    "# │ THETA PARAMETER:                                                            │\n",
    "# │   - theta = 0.0: No carryover (Search ads, immediate intent)               │\n",
    "# │   - theta = 0.5: Medium carryover (Facebook, consideration)                │\n",
    "# │   - theta = 0.8: Long carryover (LinkedIn B2B, 6-8 week cycles)            │\n",
    "# │   - theta = 0.95: Very long (TV brand campaigns, months of effect)         │\n",
    "# │                                                                             │\n",
    "# │ WHY GEOMETRIC: Simple (1 param), interpretable, matches empirical data.    │\n",
    "# │ Alternative: Weibull allows delayed peak (effect maxes at week 3).         │\n",
    "# └─────────────────────────────────────────────────────────────────────────────┘\n",
    "#\n",
    "# MATHEMATICAL INTUITION FOR ADSTOCK:\n",
    "#\n",
    "# The geometric adstock is a first-order autoregressive (AR(1)) filter:\n",
    "#   x_eff[t] = x[t] + θ·x_eff[t-1]\n",
    "#\n",
    "# Expanding recursively:\n",
    "#   x_eff[t] = x[t] + θ·x[t-1] + θ²·x[t-2] + θ³·x[t-3] + ...\n",
    "#\n",
    "# This is an infinite geometric series. For constant spend s, the steady-state is:\n",
    "#   x_eff_∞ = s·(1 + θ + θ² + ...) = s/(1-θ)\n",
    "#\n",
    "# Half-life interpretation: Effect decays to 50% after ln(0.5)/ln(θ) periods.\n",
    "#   θ=0.7 → half-life ≈ 1.9 weeks (effect halves in ~2 weeks)\n",
    "#   θ=0.9 → half-life ≈ 6.6 weeks (effect persists much longer)\n",
    "#\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "# │ 2. SATURATION / HILL FUNCTION (Diminishing Returns)                         │\n",
    "# │                                                                             │\n",
    "# │ PROBLEM: Doubling spend doesn't double revenue. At some point, you've      │\n",
    "# │ reached everyone interested. The 10th impression to the same person        │\n",
    "# │ has near-zero incremental value.                                           │\n",
    "# │                                                                             │\n",
    "# │ SOLUTION: S-curve transformation (Hill function from pharmacology):        │\n",
    "# │                                                                             │\n",
    "# │   Response │              ●●●●●●●●●●●●  ← Saturation (flat)               │\n",
    "# │      ▲     │         ●●●●●                                                 │\n",
    "# │      │     │      ●●●                    ← Efficient zone                  │\n",
    "# │      │     │    ●●                                                         │\n",
    "# │      │     │  ●●                                                           │\n",
    "# │      └─────┴──●───────────────────────► Spend                              │\n",
    "# │              gamma (half-saturation point)                                 │\n",
    "# │                                                                             │\n",
    "# │ PARAMETERS:                                                                 │\n",
    "# │   - alpha (shape): How steep the S-curve is. Higher = sharper transition.  │\n",
    "# │   - gamma (scale): Spend level where response = 50% of max.                │\n",
    "# │                    If gamma = $50k, then at $50k spend you're at 50%.      │\n",
    "# │                                                                             │\n",
    "# │ WHY HILL: Bounded [0,1], interpretable gamma, used by Robyn/LightweightMMM │\n",
    "# └─────────────────────────────────────────────────────────────────────────────┘\n",
    "#\n",
    "# MATHEMATICAL INTUITION FOR HILL SATURATION:\n",
    "#\n",
    "# The Hill function: f(x) = x^α / (x^α + γ^α)\n",
    "#\n",
    "# Key properties (useful for understanding marginal returns):\n",
    "#   - f(0) = 0, f(∞) → 1  (bounded response, asymptotes at max)\n",
    "#   - f(γ) = 0.5 exactly  (γ is the \"half-maximal effective dose\" or EC50)\n",
    "#   - Derivative: f'(x) = α·γ^α·x^(α-1) / (x^α + γ^α)²\n",
    "#     → Marginal response DECREASES as x increases (diminishing returns)\n",
    "#     → At x = γ, marginal response is α/(4γ)\n",
    "#\n",
    "# Alpha controls the \"steepness\" of the S-curve:\n",
    "#   - α < 1: Concave throughout (rapid early saturation, like commodity goods)\n",
    "#   - α = 1: Standard rectangular hyperbola f(x) = x/(x+γ) (Michaelis-Menten)\n",
    "#   - α > 1: Sigmoid with inflection point (threshold effect, then saturation)\n",
    "#            Inflection at x = γ·((α-1)/(α+1))^(1/α)\n",
    "#\n",
    "# The Hill function originates from enzyme kinetics (Michaelis-Menten) and\n",
    "# pharmacology (Hill coefficient for cooperative binding). In marketing, it\n",
    "# models audience saturation: eventually everyone who will respond, has.\n",
    "# =============================================================================\n",
    "\n",
    "def geometric_adstock(x: np.ndarray, theta: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Geometric Adstock Transformation (Carryover Effect).\n",
    "    \n",
    "    Models the \"memory\" of advertising: this week's effective spend includes\n",
    "    decayed contributions from all prior weeks. Equivalent to an infinite\n",
    "    geometric series: x_eff[t] = x[t] + θ*x[t-1] + θ²*x[t-2] + ...\n",
    "    \n",
    "    Formula: x_adstocked[t] = x[t] + theta * x_adstocked[t-1]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array - Raw spend values (weekly)\n",
    "    theta : float - Decay rate (0 to 1). The \"half-life\" is ln(0.5)/ln(θ) weeks.\n",
    "                   - LinkedIn B2B: 0.7-0.9 (long consideration cycle)\n",
    "                   - Paid Search: 0.1-0.3 (immediate intent, fast decay)\n",
    "                   - Display: 0.4-0.6 (awareness, medium decay)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x_adstocked : array - Transformed values reflecting cumulative exposure\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x_adstocked = np.zeros_like(x)\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        return x_adstocked\n",
    "    \n",
    "    # Recursive computation: each period inherits θ fraction of previous\n",
    "    x_adstocked[0] = x[0]\n",
    "    for t in range(1, len(x)):\n",
    "        x_adstocked[t] = x[t] + theta * x_adstocked[t-1]\n",
    "    \n",
    "    return x_adstocked\n",
    "\n",
    "\n",
    "def hill_saturation(x: np.ndarray, alpha: float, gamma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hill Function for Saturation (Diminishing Returns).\n",
    "    \n",
    "    Maps spend to a 0-1 scale representing \"response intensity\". At gamma spend,\n",
    "    response is exactly 0.5 (50% of maximum possible). This is the \"half-EC50\"\n",
    "    concept from pharmacology applied to marketing.\n",
    "    \n",
    "    Formula: x^α / (x^α + γ^α)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array - Adstocked spend values (apply adstock FIRST, then saturation)\n",
    "    alpha : float - Shape/slope parameter (typically 0.5 to 3.0)\n",
    "                   - alpha < 1: Concave from origin (quick saturation)\n",
    "                   - alpha = 1: Standard hyperbolic\n",
    "                   - alpha > 1: S-curve with inflection point (slow start, then steep)\n",
    "    gamma : float - Half-saturation point. Spend level where response = 50% of max.\n",
    "                   Typically set relative to observed spend range (e.g., median spend).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x_saturated : array - Values in [0, 1] representing response intensity\n",
    "    \n",
    "    Note: Final revenue contribution = coefficient × saturated_value\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = np.maximum(x, 0)  # No negative spend\n",
    "    gamma = max(gamma, 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    # Hill function: asymptotes to 1 as x → ∞\n",
    "    x_saturated = (x ** alpha) / (x ** alpha + gamma ** alpha)\n",
    "    return x_saturated\n",
    "\n",
    "\n",
    "def apply_media_transformations(\n",
    "    X: pd.DataFrame, \n",
    "    params: Dict[str, Dict[str, float]], \n",
    "    channels: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Apply adstock and saturation transformations to all media channels.\"\"\"\n",
    "    X_transformed = X.copy()\n",
    "    \n",
    "    for ch in channels:\n",
    "        if ch not in X.columns or ch not in params:\n",
    "            continue\n",
    "            \n",
    "        p = params[ch]\n",
    "        x_raw = X[ch].values\n",
    "        \n",
    "        # Step 1: Adstock (carryover)\n",
    "        x_adstocked = geometric_adstock(x_raw, p['theta'])\n",
    "        \n",
    "        # Step 2: Saturation (diminishing returns)\n",
    "        x_saturated = hill_saturation(x_adstocked, p['alpha'], p['gamma'])\n",
    "        \n",
    "        X_transformed[ch] = x_saturated\n",
    "    \n",
    "    return X_transformed\n",
    "\n",
    "# Demonstrate transformations\n",
    "print(\"Transformation functions defined.\")\n",
    "print(\"\\nExample: Geometric adstock with theta=0.7\")\n",
    "sample_spend = np.array([100, 0, 0, 0, 0, 50, 0, 0])\n",
    "sample_adstock = geometric_adstock(sample_spend, theta=0.7)\n",
    "print(f\"Raw spend:     {sample_spend}\")\n",
    "print(f\"Adstocked:     {np.round(sample_adstock, 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "pivot_for_modeling_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Pivot Data to Wide Format for Modeling\n",
    "# =============================================================================\n",
    "#\n",
    "# DATA SHAPE TRANSFORMATION:\n",
    "#\n",
    "# Input (long format):\n",
    "#   WEEK  |  CHANNEL_KEY           | SPEND  | REVENUE\n",
    "#   W1    |  LINKEDIN_EMEA_SI      | 50000  | 100000\n",
    "#   W1    |  GOOGLE_EMEA_SI        | 30000  | 100000\n",
    "#   W1    |  LINKEDIN_APAC_HC      | 20000  | 100000\n",
    "#   W2    |  LINKEDIN_EMEA_SI      | 45000  | 105000\n",
    "#   ...\n",
    "#\n",
    "# Output (wide format for regression):\n",
    "#   WEEK | LINKEDIN_EMEA_SI | GOOGLE_EMEA_SI | LINKEDIN_APAC_HC | ... | REVENUE\n",
    "#   W1   | 50000            | 30000          | 20000            | ... | 100000\n",
    "#   W2   | 45000            | 25000          | 22000            | ... | 105000\n",
    "#\n",
    "# WHY WIDE FORMAT:\n",
    "# Regression needs y ~ X1 + X2 + X3 + ...\n",
    "# Each column is a \"feature\" (channel×region×product combination)\n",
    "# Each row is an observation (week)\n",
    "#\n",
    "# MIN_SPEND_THRESHOLD:\n",
    "# Channels with < $1000 total spend are dropped because:\n",
    "#   - Not enough signal to estimate effect reliably\n",
    "#   - Adds noise and parameters without benefit\n",
    "#   - Can cause numerical instability in optimization\n",
    "# =============================================================================\n",
    "\n",
    "def pivot_for_modeling(\n",
    "    df: pd.DataFrame, \n",
    "    config: MMMConfig,\n",
    "    min_spend_threshold: float = 1000\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Pivot data to wide format for regression modeling.\n",
    "    \n",
    "    Transforms long-format data (one row per week×channel) to wide format\n",
    "    (one row per week, one column per channel). Filters out low-spend channels.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_media : DataFrame - Media spend variables (to be adstock/saturation transformed)\n",
    "    y : Series - Target variable (total revenue per week)\n",
    "    X_control : DataFrame - Control variables (seasonality, PMI, SOV)\n",
    "    channels : List - Channel keys with sufficient data for modeling\n",
    "    \"\"\"\n",
    "    # Aggregate by week and channel_key\n",
    "    df_agg = df.groupby(['WEEK_START', 'CHANNEL_KEY']).agg({\n",
    "        'SPEND': 'sum',\n",
    "        'IMPRESSIONS': 'sum',\n",
    "        'CLICKS': 'sum',\n",
    "        'REVENUE': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Pivot spend to wide format\n",
    "    X_media = df_agg.pivot_table(\n",
    "        index='WEEK_START',\n",
    "        columns='CHANNEL_KEY',\n",
    "        values='SPEND',\n",
    "        aggfunc='sum'\n",
    "    ).fillna(0).sort_index()\n",
    "    \n",
    "    # Filter channels with minimum spend\n",
    "    channel_totals = X_media.sum()\n",
    "    valid_channels = channel_totals[channel_totals >= min_spend_threshold].index.tolist()\n",
    "    X_media = X_media[valid_channels]\n",
    "    \n",
    "    # Target: Total revenue per week\n",
    "    y = df.groupby('WEEK_START')['REVENUE'].sum().sort_index()\n",
    "    \n",
    "    # Control variables\n",
    "    control_cols = ['TREND', 'SIN_1', 'COS_1', 'SIN_2', 'COS_2', 'Q1_FLAG', 'Q3_FLAG']\n",
    "    if 'PMI_INDEX' in df.columns:\n",
    "        control_cols.append('PMI_INDEX')\n",
    "    if 'COMPETITOR_SOV' in df.columns:\n",
    "        control_cols.append('COMPETITOR_SOV')\n",
    "    \n",
    "    X_control = df.groupby('WEEK_START')[control_cols].first().sort_index()\n",
    "    \n",
    "    # Align indices\n",
    "    common_idx = X_media.index.intersection(y.index).intersection(X_control.index)\n",
    "    X_media = X_media.loc[common_idx]\n",
    "    y = y.loc[common_idx]\n",
    "    X_control = X_control.loc[common_idx]\n",
    "    \n",
    "    channels = X_media.columns.tolist()\n",
    "    \n",
    "    return X_media, y, X_control, channels\n",
    "\n",
    "# Pivot data\n",
    "X_media, y, X_control, channels = pivot_for_modeling(df, config)\n",
    "\n",
    "print(f\"\\nModeling {len(channels)} channel-region-product combinations\")\n",
    "print(f\"Time periods: {len(y)} weeks\")\n",
    "print(f\"Total spend: ${X_media.sum().sum():,.0f}\")\n",
    "print(f\"Total revenue: ${y.sum():,.0f}\")\n",
    "print(f\"\\nControl variables: {X_control.columns.tolist()}\")\n",
    "print(f\"\\nTop 10 channels by spend:\")\n",
    "print(X_media.sum().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cv_and_metrics_functions_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Time-Series Cross-Validation\n",
    "# =============================================================================\n",
    "#\n",
    "# WHY TIME-SERIES CV INSTEAD OF K-FOLD:\n",
    "#\n",
    "# Standard k-fold CV randomly shuffles data, which would let us \"peek\" at future\n",
    "# weeks when predicting past weeks. This causes overly optimistic metrics because\n",
    "# the model learns patterns it wouldn't have access to in production.\n",
    "#\n",
    "# Time-series CV respects temporal order:\n",
    "#\n",
    "#   Fold 1: Train [Week 1-52]  → Test [Week 53-65]   (predict Q1 2024)\n",
    "#   Fold 2: Train [Week 14-65] → Test [Week 66-78]  (predict Q2 2024)\n",
    "#   Fold 3: Train [Week 27-78] → Test [Week 79-91]  (predict Q3 2024)\n",
    "#   ...\n",
    "#\n",
    "# This mimics real use: \"Given everything up to today, how well can we predict\n",
    "# next quarter?\" If CV MAPE is 12%, expect 12% error in actual forecasts.\n",
    "#\n",
    "# METRIC CHOICES:\n",
    "# - MAPE (Mean Absolute Percentage Error): \"On average, we're off by X%\"\n",
    "#   Industry standard for MMM. Target: <15% is good, <10% is excellent.\n",
    "# - NRMSE: RMSE normalized by mean. Comparable across different revenue scales.\n",
    "# - R²: Variance explained. >0.85 for in-sample, >0.70 for CV is solid.\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEEPER DIVE: UNDERSTANDING EACH METRIC\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# R² (Coefficient of Determination):\n",
    "#   R² = 1 - SS_res / SS_tot = 1 - Σ(y - ŷ)² / Σ(y - ȳ)²\n",
    "#\n",
    "#   Interpretation: \"What fraction of the variance in y does our model explain?\"\n",
    "#   - R² = 1.0: Perfect predictions (ŷ = y for all points)\n",
    "#   - R² = 0.0: Model predicts the mean every time (useless)\n",
    "#   - R² < 0:   Model is WORSE than predicting the mean (possible in CV!)\n",
    "#\n",
    "#   Caution: R² can be artificially high if y has a strong trend. A model that\n",
    "#   just predicts \"revenue goes up 5% per year\" might get R² = 0.8 without\n",
    "#   capturing any marketing effects. That's why we include TREND as a control.\n",
    "#\n",
    "# RMSE (Root Mean Squared Error):\n",
    "#   RMSE = √[Σ(y - ŷ)² / n]\n",
    "#\n",
    "#   Same units as y (dollars). Penalizes large errors heavily due to squaring.\n",
    "#   Good for: \"On average, how many dollars off are we?\"\n",
    "#   Weakness: Not comparable across different revenue scales.\n",
    "#\n",
    "# MAE (Mean Absolute Error):\n",
    "#   MAE = Σ|y - ŷ| / n\n",
    "#\n",
    "#   More robust to outliers than RMSE (no squaring). Also in dollars.\n",
    "#   If MAE << RMSE, you have some big outliers (worth investigating).\n",
    "#\n",
    "# MAPE (Mean Absolute Percentage Error):\n",
    "#   MAPE = (1/n) · Σ|y - ŷ| / |y| × 100\n",
    "#\n",
    "#   The \"headline\" metric for business stakeholders.\n",
    "#   - Scale-free (%) so comparable across regions/products\n",
    "#   - Intuitive: \"We're typically 12% off\"\n",
    "#   - Weakness: Undefined when y = 0 (we mask those out)\n",
    "#   - Weakness: Asymmetric—50% under-prediction feels same as 100% over-prediction\n",
    "#\n",
    "# NRMSE (Normalized RMSE):\n",
    "#   NRMSE = RMSE / mean(y) × 100\n",
    "#\n",
    "#   Percentage scale like MAPE, but uses RMSE instead of MAE.\n",
    "#   Useful for comparing model quality across different datasets.\n",
    "#\n",
    "# WHY WE REPORT MULTIPLE METRICS:\n",
    "# No single metric tells the whole story. R² shows explanatory power, MAPE\n",
    "# shows practical accuracy, RMSE reveals if large errors exist. Together they\n",
    "# give a complete picture of model quality.\n",
    "# =============================================================================\n",
    "\n",
    "def time_series_cv_split(\n",
    "    n_samples: int,\n",
    "    train_size: int,\n",
    "    test_size: int,\n",
    "    step_size: int\n",
    ") -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Generate time-series cross-validation splits (rolling window).\n",
    "    \n",
    "    Unlike k-fold, this NEVER lets the model see future data during training.\n",
    "    Each fold trains on [t, t+train_size) and tests on [t+train_size, t+train_size+test_size).\n",
    "    \n",
    "    With train=52, test=13, step=13:\n",
    "    - ~4 folds per 2 years of data\n",
    "    - Each fold predicts a full quarter ahead\n",
    "    - Realistic for \"next quarter budget planning\" use case\n",
    "    \"\"\"\n",
    "    splits = []\n",
    "    start = 0\n",
    "    while start + train_size + test_size <= n_samples:\n",
    "        train_idx = np.arange(start, start + train_size)\n",
    "        test_idx = np.arange(start + train_size, start + train_size + test_size)\n",
    "        splits.append((train_idx, test_idx))\n",
    "        start += step_size\n",
    "    return splits\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate regression metrics for model evaluation.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - R²: Proportion of variance explained (1.0 = perfect, can be negative if worse than mean)\n",
    "    - RMSE: Root Mean Squared Error in dollars (same units as y)\n",
    "    - MAE: Mean Absolute Error in dollars (less sensitive to outliers than RMSE)\n",
    "    - MAPE: Mean Absolute Percentage Error (the \"headline\" metric for MMM)\n",
    "    - NRMSE: Normalized RMSE as % of mean (allows comparison across scales)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    mask = y_true != 0  # Avoid division by zero in MAPE\n",
    "    \n",
    "    return {\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.sum() > 0 else np.nan,\n",
    "        'NRMSE': np.sqrt(mean_squared_error(y_true, y_pred)) / y_true.mean() * 100\n",
    "    }\n",
    "\n",
    "# Generate CV splits\n",
    "cv_splits = time_series_cv_split(\n",
    "    n_samples=len(y),\n",
    "    train_size=config.cv_train_weeks,\n",
    "    test_size=config.cv_test_weeks,\n",
    "    step_size=config.cv_step_weeks\n",
    ")\n",
    "\n",
    "print(f\"Time-Series Cross-Validation:\")\n",
    "print(f\"  Training window: {config.cv_train_weeks} weeks\")\n",
    "print(f\"  Test window: {config.cv_test_weeks} weeks\")\n",
    "print(f\"  Number of folds: {len(cv_splits)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "mmm_optimizer_class_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Hyperparameter Optimization with Nevergrad\n",
    "# =============================================================================\n",
    "#\n",
    "# THE OPTIMIZATION PROBLEM:\n",
    "#\n",
    "# We need to find 3 hyperparameters PER CHANNEL:\n",
    "#   - theta (adstock decay): How quickly does ad effect fade?\n",
    "#   - alpha (saturation shape): How steep is the diminishing returns curve?\n",
    "#   - gamma (half-saturation): At what spend level do we hit 50% of max response?\n",
    "#\n",
    "# With 20 channels, that's 60 parameters. We can't grid search (60^10 = impossible).\n",
    "# We can't use gradient descent (the objective isn't smooth w.r.t. these params).\n",
    "#\n",
    "# SOLUTION: Evolutionary optimization (Nevergrad's TwoPointsDE)\n",
    "#\n",
    "# TwoPointsDE is a variant of Differential Evolution that:\n",
    "#   1. Starts with a population of random parameter guesses\n",
    "#   2. \"Breeds\" new guesses by combining good performers\n",
    "#   3. Keeps the best, discards the worst\n",
    "#   4. Repeats for `budget` iterations\n",
    "#\n",
    "# WHY 500 ITERATIONS: Empirically, loss stabilizes around 300-500 for this scale.\n",
    "# Robyn uses 2000+ but also optimizes decomposition. We're simpler.\n",
    "#\n",
    "# SIGMOID REPARAMETRIZATION:\n",
    "# Instead of letting optimizer search [0, 0.95] directly, we search [-5, 5] and\n",
    "# apply sigmoid to map to the valid range. This is a standard trick to:\n",
    "#   - Avoid boundary issues (optimizer doesn't get stuck at 0 or 0.95)\n",
    "#   - Make the search space more \"smooth\" for the evolutionary algorithm\n",
    "#\n",
    "# NEGATIVE COEFFICIENT PENALTY:\n",
    "# Economically, marketing should never HURT revenue. If the model wants to assign\n",
    "# a negative coefficient (e.g., \"more LinkedIn spend = less revenue\"), we penalize\n",
    "# this heavily. In practice, negative coefficients usually mean multicollinearity.\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEEPER DIVE: WHY DERIVATIVE-FREE OPTIMIZATION?\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# The objective function L(θ, α, γ) = 1 - R²(model trained with those params)\n",
    "# involves FITTING A RIDGE REGRESSION INSIDE each evaluation. This creates\n",
    "# a \"nested\" optimization that makes gradients unavailable or meaningless:\n",
    "#\n",
    "#   ∂L/∂θ = ??? (how does R² change if we nudge decay by 0.01?)\n",
    "#\n",
    "# The relationship is:\n",
    "#   θ → adstock_transform(X) → Ridge.fit(X_transformed) → R²\n",
    "#\n",
    "# Each step involves discrete choices (which data points are \"influential\"),\n",
    "# matrix inversions, and nonlinear transforms. Autodiff doesn't help here.\n",
    "#\n",
    "# TwoPointsDE (Differential Evolution variant):\n",
    "# - Maintains a population of ~20-50 candidate solutions\n",
    "# - Creates new candidates via: x_new = x_a + F·(x_b - x_c) + noise\n",
    "#   where F is a mutation factor and a,b,c are random population members\n",
    "# - \"TwoPoints\" variant uses 2 random points for crossover, improving\n",
    "#   exploitation vs. exploration balance\n",
    "# - No gradients needed—just function evaluations and selection\n",
    "#\n",
    "# Alternatives considered:\n",
    "# - Bayesian Optimization: Better for <20 params, but O(n³) with iterations\n",
    "# - Random Search: Surprisingly effective, but needs 10x more iterations\n",
    "# - Hyperband: Good for early stopping, but our objective is fast to evaluate\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# RIDGE REGRESSION: WHY L2 PENALTY?\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# Standard OLS minimizes: ||y - Xβ||²\n",
    "# Ridge adds:             ||y - Xβ||² + λ||β||²\n",
    "#\n",
    "# The L2 penalty (λ||β||²) does two things:\n",
    "# 1. REGULARIZATION: Shrinks coefficients toward zero, preventing overfitting\n",
    "#    when n (samples) is small relative to p (features). With 104 weeks and\n",
    "#    20+ channels, we're in moderate-dimensional territory.\n",
    "#\n",
    "# 2. MULTICOLLINEARITY FIX: When channels are correlated (LinkedIn and Display\n",
    "#    spike together in Q4), (X'X) is near-singular. Ridge adds λI to the diagonal:\n",
    "#    β_ridge = (X'X + λI)⁻¹X'y, which is always invertible.\n",
    "#\n",
    "# Why NOT Lasso (L1)? Lasso sets some coefficients exactly to zero, which is\n",
    "# great for feature selection but problematic here—we WANT every channel's\n",
    "# contribution estimated, even if small. Zeroing LinkedIn would lose insights.\n",
    "#\n",
    "# Why NOT ElasticNet? Adds complexity without clear benefit for our use case.\n",
    "# Ridge's closed-form solution is also computationally efficient.\n",
    "# =============================================================================\n",
    "\n",
    "class MMMOptimizer:\n",
    "    \"\"\"\n",
    "    Marketing Mix Model optimizer using Nevergrad evolutionary algorithm.\n",
    "    \n",
    "    Finds optimal (theta, alpha, gamma) for each channel by minimizing (1 - R²)\n",
    "    with a penalty for economically invalid negative coefficients.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_media, X_control, y, channels, config):\n",
    "        self.X_media = X_media\n",
    "        self.X_control = X_control\n",
    "        self.y = y\n",
    "        self.channels = channels\n",
    "        self.config = config\n",
    "        self.n_params = len(channels) * 3  # 3 params per channel\n",
    "        # Store max spend per channel for gamma scaling\n",
    "        self.channel_max = {ch: max(X_media[ch].max(), 1) for ch in channels}\n",
    "        \n",
    "    def _decode_params(self, flat_params):\n",
    "        \"\"\"\n",
    "        Decode flat parameter array into structured dict.\n",
    "        \n",
    "        Uses sigmoid transform to map unbounded search space [-5, 5] to valid ranges:\n",
    "        - theta: [0, 0.95] (can't be 1.0 or adstock explodes)\n",
    "        - alpha: [0.5, 3.0] (reasonable S-curve shapes)\n",
    "        - gamma: [0, max_spend] (scaled to channel's observed range)\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for i, ch in enumerate(self.channels):\n",
    "            base = i * 3\n",
    "            raw_theta, raw_alpha, raw_gamma = flat_params[base:base+3]\n",
    "            \n",
    "            # Sigmoid: 1/(1+e^-x) maps (-∞,∞) → (0,1), then scale to target range\n",
    "            theta = 1 / (1 + np.exp(-raw_theta)) * 0.95  # [0, 0.95]\n",
    "            alpha = 0.5 + 1 / (1 + np.exp(-raw_alpha)) * 2.5  # [0.5, 3.0]\n",
    "            gamma = 1 / (1 + np.exp(-raw_gamma)) * self.channel_max[ch]  # [0, max]\n",
    "            \n",
    "            params[ch] = {'theta': theta, 'alpha': alpha, 'gamma': max(gamma, 1e-6)}\n",
    "        return params\n",
    "    \n",
    "    def _objective(self, flat_params):\n",
    "        \"\"\"\n",
    "        Objective function: Minimize (1 - R²) + penalty for negative coefficients.\n",
    "        \n",
    "        Why (1 - R²)? We want to MAXIMIZE R², but optimizers MINIMIZE.\n",
    "        So we minimize (1 - R²), which is 0 when R² = 1 (perfect fit).\n",
    "        \n",
    "        Why the penalty? Marketing spend should never decrease revenue.\n",
    "        A negative coefficient means multicollinearity or data issues.\n",
    "        Heavy penalty (10x squared) pushes optimizer toward valid solutions.\n",
    "        \"\"\"\n",
    "        params = self._decode_params(flat_params)\n",
    "        X_media_trans = apply_media_transformations(self.X_media, params, self.channels)\n",
    "        X_full = pd.concat([X_media_trans, self.X_control], axis=1)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_full)\n",
    "        \n",
    "        model = Ridge(alpha=self.config.ridge_alpha)\n",
    "        model.fit(X_scaled, self.y)\n",
    "        y_pred = model.predict(X_scaled)\n",
    "        \n",
    "        # Penalize negative media coefficients (economically invalid)\n",
    "        media_coefs = model.coef_[:len(self.channels)]\n",
    "        negative_penalty = np.sum(np.minimum(media_coefs, 0) ** 2) * 10\n",
    "        \n",
    "        r2 = r2_score(self.y, y_pred)\n",
    "        return (1 - r2) + negative_penalty\n",
    "    \n",
    "    def optimize(self, budget=500):\n",
    "        \"\"\"\n",
    "        Run Nevergrad optimization.\n",
    "        \n",
    "        TwoPointsDE (Two-Points Differential Evolution):\n",
    "        - Population-based evolutionary algorithm\n",
    "        - Creates new candidates by combining existing good solutions\n",
    "        - Robust to non-smooth, non-convex objective landscapes\n",
    "        - 500 iterations typically sufficient for 50-100 parameters\n",
    "        \"\"\"\n",
    "        print(f\"\\nOptimizing {self.n_params} parameters ({len(self.channels)} channels × 3 params)...\")\n",
    "        \n",
    "        # Search space: unbounded, will be mapped via sigmoid in _decode_params\n",
    "        parametrization = ng.p.Array(shape=(self.n_params,)).set_bounds(-5, 5)\n",
    "        optimizer = ng.optimizers.TwoPointsDE(parametrization=parametrization, budget=budget)\n",
    "        recommendation = optimizer.minimize(self._objective)\n",
    "        \n",
    "        best_params = self._decode_params(recommendation.value)\n",
    "        final_loss = self._objective(recommendation.value)\n",
    "        \n",
    "        print(f\"Optimization complete. Final loss: {final_loss:.4f} (R² ≈ {1 - final_loss:.4f})\")\n",
    "        return best_params, {'final_loss': final_loss}\n",
    "\n",
    "# Run optimization\n",
    "optimizer = MMMOptimizer(X_media, X_control, y, channels, config)\n",
    "best_params, opt_metrics = optimizer.optimize(budget=config.nevergrad_budget)\n",
    "\n",
    "print(f\"\\nSample optimized parameters (first 5 channels):\")\n",
    "for ch in list(best_params.keys())[:5]:\n",
    "    p = best_params[ch]\n",
    "    print(f\"  {ch}: theta={p['theta']:.3f}, alpha={p['alpha']:.3f}, gamma={p['gamma']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "train_final_model_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Final Model Training with Cross-Validation Metrics\n",
    "# =============================================================================\n",
    "#\n",
    "# TWO SEPARATE FITS:\n",
    "#\n",
    "# 1. IN-SAMPLE FIT (Full Data):\n",
    "#    - Train on ALL data, predict on ALL data\n",
    "#    - R² will be high (0.90+) because model has \"seen\" every data point\n",
    "#    - Use for: coefficient interpretation, response curves, budget optimization\n",
    "#\n",
    "# 2. CROSS-VALIDATION FIT (Rolling Window):\n",
    "#    - Train on past, predict on future, repeat\n",
    "#    - R² and MAPE will be WORSE than in-sample (this is expected!)\n",
    "#    - Use for: realistic accuracy estimate, \"will this work in production?\"\n",
    "#\n",
    "# WHY REPORT BOTH:\n",
    "#   - If in-sample R² = 0.95 but CV R² = 0.50, model is OVERFITTING\n",
    "#     (memorizing training data, not learning generalizable patterns)\n",
    "#   - Healthy gap: in-sample R² ≈ CV R² + 0.05-0.10\n",
    "#   - If gap is large: reduce model complexity (fewer channels, stronger ridge penalty)\n",
    "#\n",
    "# QUALITY THRESHOLDS (industry standard):\n",
    "#   CV MAPE < 10%: Excellent - model is highly predictive\n",
    "#   CV MAPE 10-20%: Good - suitable for budget optimization\n",
    "#   CV MAPE 20-30%: Acceptable - directional insights only\n",
    "#   CV MAPE > 30%: Poor - investigate data quality or model specification\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEEPER DIVE: BIAS-VARIANCE TRADEOFF IN MMM\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# The gap between in-sample and CV performance illustrates the bias-variance\n",
    "# tradeoff central to all statistical learning:\n",
    "#\n",
    "#   Expected Error = Bias² + Variance + Irreducible Noise\n",
    "#\n",
    "# BIAS: Systematic error from model assumptions (e.g., assuming linearity when\n",
    "#       relationships are nonlinear). High bias = underfitting.\n",
    "#\n",
    "# VARIANCE: Sensitivity to the specific training data. A complex model might\n",
    "#           fit training data perfectly but fail on new data. High variance = overfitting.\n",
    "#\n",
    "# IN-SAMPLE ERROR captures (mostly) bias: if the model can't fit the training\n",
    "# data well, it has too little flexibility. Low in-sample error means the model\n",
    "# can represent the data's complexity.\n",
    "#\n",
    "# CV ERROR captures bias + variance: on unseen data, both systematic errors AND\n",
    "# overfitting manifest. The GAP between in-sample and CV is (roughly) variance.\n",
    "#\n",
    "# HOW RIDGE HELPS:\n",
    "# Ridge penalty λ||β||² adds bias (shrinks coefficients toward zero) but reduces\n",
    "# variance (prevents wild coefficient estimates from correlated features).\n",
    "#\n",
    "# If CV performance is poor despite good in-sample fit:\n",
    "#   - Increase λ (more regularization) to reduce variance\n",
    "#   - Reduce model complexity (fewer channels, simpler transforms)\n",
    "#   - Get more data (especially more time periods)\n",
    "#\n",
    "# If both in-sample AND CV are poor:\n",
    "#   - Model may be too simple (underfitting)\n",
    "#   - Check data quality (missing spend, misaligned time series)\n",
    "#   - Consider adding control variables (macro factors, seasonality)\n",
    "#\n",
    "# THE ±STD IN CV METRICS:\n",
    "# We report mean ± std across folds. High std indicates inconsistent performance\n",
    "# across time periods. This could mean:\n",
    "#   - Some quarters are inherently harder to predict (Q4 chaos)\n",
    "#   - Structural breaks (pandemic, new product launch)\n",
    "#   - Concept drift (marketing effectiveness changing over time)\n",
    "# =============================================================================\n",
    "\n",
    "def train_final_model(X_media, X_control, y, channels, params, cv_splits, config):\n",
    "    \"\"\"\n",
    "    Train final model and compute both in-sample and cross-validation metrics.\n",
    "    \n",
    "    In-sample metrics show model fit; CV metrics show predictive accuracy.\n",
    "    Large gap between them indicates overfitting.\n",
    "    \"\"\"\n",
    "    # Transform media with optimized hyperparameters\n",
    "    X_media_trans = apply_media_transformations(X_media, params, channels)\n",
    "    X_full = pd.concat([X_media_trans, X_control], axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_full)\n",
    "    \n",
    "    # In-Sample Fit\n",
    "    model = Ridge(alpha=config.ridge_alpha)\n",
    "    model.fit(X_scaled, y)\n",
    "    y_pred_insample = model.predict(X_scaled)\n",
    "    insample_metrics = calculate_metrics(y.values, y_pred_insample)\n",
    "    \n",
    "    # Cross-Validation\n",
    "    cv_metrics_list = []\n",
    "    y_values = y.values\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_values[train_idx], y_values[test_idx]\n",
    "        \n",
    "        cv_model = Ridge(alpha=config.ridge_alpha)\n",
    "        cv_model.fit(X_train, y_train)\n",
    "        y_pred = cv_model.predict(X_test)\n",
    "        \n",
    "        fold_metrics = calculate_metrics(y_test, y_pred)\n",
    "        fold_metrics['fold'] = fold_idx + 1\n",
    "        cv_metrics_list.append(fold_metrics)\n",
    "    \n",
    "    cv_metrics_df = pd.DataFrame(cv_metrics_list)\n",
    "    cv_metrics_avg = cv_metrics_df.drop('fold', axis=1).mean().to_dict()\n",
    "    cv_metrics_std = cv_metrics_df.drop('fold', axis=1).std().to_dict()\n",
    "    \n",
    "    metrics = {\n",
    "        'in_sample': insample_metrics,\n",
    "        'cv_mean': cv_metrics_avg,\n",
    "        'cv_std': cv_metrics_std\n",
    "    }\n",
    "    \n",
    "    return model, scaler, X_full, metrics\n",
    "\n",
    "# Train final model\n",
    "model, scaler, X_transformed, metrics = train_final_model(\n",
    "    X_media, X_control, y, channels, best_params, cv_splits, config\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nIn-Sample (Full Data):\")\n",
    "for metric, value in metrics['in_sample'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nCross-Validation (Out-of-Sample):\")\n",
    "for metric in ['R2', 'MAPE', 'NRMSE']:\n",
    "    mean_val = metrics['cv_mean'].get(metric, 0)\n",
    "    std_val = metrics['cv_std'].get(metric, 0)\n",
    "    print(f\"  {metric}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "# Model quality assessment\n",
    "cv_mape = metrics['cv_mean'].get('MAPE', 100)\n",
    "if cv_mape < 10:\n",
    "    quality = \"EXCELLENT\"\n",
    "elif cv_mape < 20:\n",
    "    quality = \"GOOD\"\n",
    "elif cv_mape < 30:\n",
    "    quality = \"ACCEPTABLE\"\n",
    "else:\n",
    "    quality = \"NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"\\nModel Quality: {quality} (CV MAPE = {cv_mape:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "bootstrap_roi_confidence_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Bootstrap Confidence Intervals for ROI\n",
    "# =============================================================================\n",
    "#\n",
    "# WHY BOOTSTRAP INSTEAD OF ANALYTIC CONFIDENCE INTERVALS:\n",
    "#\n",
    "# Traditional CI formulas assume:\n",
    "#   - Normally distributed errors\n",
    "#   - Independent observations\n",
    "#   - Linear relationships\n",
    "#\n",
    "# MMM violates all three:\n",
    "#   - Errors are often heteroscedastic (bigger in Q4)\n",
    "#   - Time series has autocorrelation (this week's revenue predicts next week's)\n",
    "#   - We applied nonlinear transforms (adstock, saturation)\n",
    "#\n",
    "# BOOTSTRAP APPROACH:\n",
    "#   1. Resample the data WITH REPLACEMENT (some weeks appear twice, some not at all)\n",
    "#   2. Re-fit the model on this \"fake\" dataset\n",
    "#   3. Calculate ROI for each channel\n",
    "#   4. Repeat 100 times\n",
    "#   5. The 5th and 95th percentiles of these 100 ROIs = 90% confidence interval\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - ROI = 3.2 [2.8, 3.6] means: \"We estimate LinkedIn returns $3.20 per dollar,\n",
    "#     and we're 90% confident the true value is between $2.80 and $3.60\"\n",
    "#   - IS_SIGNIFICANT = True means the entire CI is above zero (we're confident\n",
    "#     the channel has positive ROI, not just statistical noise)\n",
    "#\n",
    "# NOTE: We keep adstock/saturation params FIXED during bootstrap. We're quantifying\n",
    "# uncertainty in the COEFFICIENTS, not the hyperparameters. Full uncertainty would\n",
    "# require nested optimization (computationally prohibitive).\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEEPER DIVE: THE BOOTSTRAP PRINCIPLE\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# The key insight (Efron, 1979): The empirical distribution of your sample\n",
    "# approximates the true population distribution. By resampling FROM your sample,\n",
    "# you simulate what WOULD happen if you could re-run the entire data collection.\n",
    "#\n",
    "# For a statistic θ̂ (like ROI), the bootstrap distribution of θ̂* approximates\n",
    "# the sampling distribution of θ̂. The standard error of θ̂* over B bootstrap\n",
    "# samples estimates the true standard error of θ̂.\n",
    "#\n",
    "# PERCENTILE METHOD (what we use):\n",
    "# The (α/2, 1-α/2) percentiles of the bootstrap distribution give a (1-α) CI.\n",
    "# For 90% CI: we take the 5th and 95th percentiles of 100 bootstrap ROIs.\n",
    "#\n",
    "# ALTERNATIVE METHODS (not used here, but worth knowing):\n",
    "# - BCa (Bias-Corrected Accelerated): Adjusts for skewness and bias in θ̂\n",
    "# - Studentized Bootstrap: Divides by bootstrap SE, more accurate for small n\n",
    "# - Block Bootstrap: For time series—resamples contiguous blocks to preserve\n",
    "#   autocorrelation. We don't use this because our primary goal is coefficient\n",
    "#   uncertainty, and time-series structure is less critical for that.\n",
    "#\n",
    "# WHY 100 ITERATIONS:\n",
    "# - SE of a percentile estimate ≈ √(p(1-p)/B) where p is the percentile\n",
    "# - For p=0.05 and B=100: SE ≈ 0.022 (good enough for practical decisions)\n",
    "# - B=1000 would give SE ≈ 0.007 (diminishing returns for 10x compute)\n",
    "#\n",
    "# COEFFICIENT UNSCALING:\n",
    "# Note: We divide by scaler.scale_ to convert back to original units.\n",
    "# StandardScaler transforms: X_scaled = (X - μ) / σ\n",
    "# Ridge fits: y = Σ β_scaled[i] · X_scaled[i]\n",
    "# To get interpretable coefficients: β_original[i] = β_scaled[i] / σ[i]\n",
    "# =============================================================================\n",
    "\n",
    "def bootstrap_roi_confidence(X_media, X_control, y, channels, params, config):\n",
    "    \"\"\"\n",
    "    Bootstrap confidence intervals for channel ROI estimates.\n",
    "    \n",
    "    Resamples data 100 times, re-fits model each time, collects distribution\n",
    "    of ROI estimates. This captures uncertainty from:\n",
    "      - Sample variability (different weeks have different patterns)\n",
    "      - Coefficient estimation (regression has standard errors)\n",
    "    \n",
    "    Does NOT capture uncertainty in hyperparameters (theta, alpha, gamma).\n",
    "    \"\"\"\n",
    "    n_samples = len(y)\n",
    "    n_bootstrap = config.n_bootstrap\n",
    "    ci_level = config.confidence_level\n",
    "    \n",
    "    # Apply transformations once (params are fixed)\n",
    "    X_media_trans = apply_media_transformations(X_media, params, channels)\n",
    "    X_full = pd.concat([X_media_trans, X_control], axis=1)\n",
    "    \n",
    "    roi_samples = {ch: [] for ch in channels}\n",
    "    coef_samples = {ch: [] for ch in channels}\n",
    "    \n",
    "    print(f\"\\nRunning {n_bootstrap} bootstrap iterations for {ci_level*100:.0f}% CI...\")\n",
    "    \n",
    "    for b in range(n_bootstrap):\n",
    "        boot_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        X_boot = X_full.iloc[boot_idx]\n",
    "        y_boot = y.iloc[boot_idx]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_boot)\n",
    "        \n",
    "        model = Ridge(alpha=config.ridge_alpha)\n",
    "        model.fit(X_scaled, y_boot)\n",
    "        \n",
    "        coefs = model.coef_ / scaler.scale_\n",
    "        \n",
    "        for i, ch in enumerate(channels):\n",
    "            coef = coefs[i]\n",
    "            coef_samples[ch].append(coef)\n",
    "            \n",
    "            contribution = X_media_trans[ch].iloc[boot_idx].sum() * coef\n",
    "            spend = X_media[ch].iloc[boot_idx].sum()\n",
    "            roi = contribution / spend if spend > 0 else 0\n",
    "            roi_samples[ch].append(roi)\n",
    "        \n",
    "        if (b + 1) % 25 == 0:\n",
    "            print(f\"  Completed {b + 1}/{n_bootstrap}\")\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    alpha = 1 - ci_level\n",
    "    results = []\n",
    "    \n",
    "    for ch in channels:\n",
    "        rois = np.array(roi_samples[ch])\n",
    "        coefs = np.array(coef_samples[ch])\n",
    "        ci_key = f'ROI_CI_{int(ci_level*100)}'\n",
    "        \n",
    "        results.append({\n",
    "            'CHANNEL_KEY': ch,\n",
    "            'ROI_MEAN': np.mean(rois),\n",
    "            'ROI_MEDIAN': np.median(rois),\n",
    "            'ROI_STD': np.std(rois),\n",
    "            f'ROI_CI_LOWER_{int(ci_level*100)}': np.percentile(rois, alpha/2 * 100),\n",
    "            f'ROI_CI_UPPER_{int(ci_level*100)}': np.percentile(rois, (1 - alpha/2) * 100),\n",
    "            'COEF_MEAN': np.mean(coefs),\n",
    "            'COEF_STD': np.std(coefs),\n",
    "            'TOTAL_SPEND': X_media[ch].sum()\n",
    "        })\n",
    "    \n",
    "    roi_ci = pd.DataFrame(results)\n",
    "    roi_ci['IS_SIGNIFICANT'] = roi_ci[f'ROI_CI_LOWER_{int(ci_level*100)}'] > 0\n",
    "    \n",
    "    return roi_ci.sort_values('ROI_MEAN', ascending=False)\n",
    "\n",
    "# Run bootstrap\n",
    "roi_confidence = bootstrap_roi_confidence(\n",
    "    X_media, X_control, y, channels, best_params, config\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHANNEL ROI WITH CONFIDENCE INTERVALS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConfidence Level: {config.confidence_level*100:.0f}%\")\n",
    "print(\"\\nTop 10 Channels by ROI:\")\n",
    "display_cols = ['CHANNEL_KEY', 'ROI_MEAN', f'ROI_CI_LOWER_{int(config.confidence_level*100)}', \n",
    "                f'ROI_CI_UPPER_{int(config.confidence_level*100)}', 'IS_SIGNIFICANT', 'TOTAL_SPEND']\n",
    "print(roi_confidence[display_cols].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_response_curves_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Generate Response Curves and Marginal ROI\n",
    "# =============================================================================\n",
    "#\n",
    "# RESPONSE CURVES: THE KEY VISUALIZATION\n",
    "#\n",
    "# A response curve shows predicted revenue contribution as a function of spend:\n",
    "#\n",
    "#   Revenue │              ●●●●●●●●●●●●  ← Saturation zone (mROI < 1)\n",
    "#   Contrib │         ●●●●●               Each extra $ returns < $1\n",
    "#     ▲     │      ●●●                    = \"Wasted spend\"\n",
    "#     │     │    ●●\n",
    "#     │     │  ●●                       ← Efficient zone (mROI > 1)\n",
    "#     │     │●●                           Each extra $ returns > $1\n",
    "#     └─────┴──────────────────────────► Spend\n",
    "#           0      $50k     $100k\n",
    "#\n",
    "# AVERAGE ROI vs. MARGINAL ROI:\n",
    "#\n",
    "#   Average ROI = Total contribution / Total spend\n",
    "#               = \"What did we get back per dollar HISTORICALLY?\"\n",
    "#\n",
    "#   Marginal ROI = d(contribution)/d(spend) at CURRENT spend level\n",
    "#                = \"What will we get back for the NEXT dollar?\"\n",
    "#\n",
    "# MARGINAL ROI IS MORE USEFUL because:\n",
    "#   - A channel with 5x average ROI might be saturated (0.5x marginal)\n",
    "#   - A channel with 2x average ROI might have headroom (3x marginal)\n",
    "#   - Budget decisions are about the NEXT dollar, not past dollars\n",
    "#\n",
    "# HOW WE CALCULATE MARGINAL ROI:\n",
    "#   Numerical derivative: [f(x + δ) - f(x)] / δ\n",
    "#   where f(x) = coefficient × hill_saturation(adstock(x))\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEEPER DIVE: THE CALCULUS OF MARGINAL ROI\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# The full response function is a composition of three transforms:\n",
    "#\n",
    "#   R(s) = β · H(A(s))\n",
    "#\n",
    "# where:\n",
    "#   s = weekly spend\n",
    "#   A(s) = s/(1-θ) = steady-state adstock (geometric series limit)\n",
    "#   H(x) = x^α / (x^α + γ^α) = Hill saturation function\n",
    "#   β = regression coefficient (revenue per unit of saturated adstock)\n",
    "#\n",
    "# The marginal ROI is dR/ds, applying chain rule:\n",
    "#\n",
    "#   dR/ds = β · dH/dA · dA/ds\n",
    "#\n",
    "#   dA/ds = 1/(1-θ)  (linear relationship at steady state)\n",
    "#\n",
    "#   dH/dA = α·γ^α·A^(α-1) / (A^α + γ^α)²  (Hill function derivative)\n",
    "#\n",
    "# Substituting:\n",
    "#   mROI = β · [α·γ^α·A^(α-1) / (A^α + γ^α)²] · [1/(1-θ)]\n",
    "#\n",
    "# KEY INSIGHT: As A → ∞ (high spend), (A^α + γ^α)² grows as A^(2α), but the\n",
    "# numerator only grows as A^(α-1). Net effect: mROI → 0 as spend → ∞.\n",
    "# This is diminishing returns made explicit through calculus.\n",
    "#\n",
    "# AT THE HALF-SATURATION POINT (A = γ):\n",
    "#   H(γ) = 0.5, and dH/dA = α/(4γ)\n",
    "#   mROI = β · α/(4γ) · 1/(1-θ)\n",
    "#\n",
    "# This gives us a quick diagnostic: if mROI at current spend is close to\n",
    "# α·β/(4γ(1-θ)), we're roughly at the \"efficient frontier\" of the S-curve.\n",
    "#\n",
    "# NUMERICAL VS. ANALYTIC DERIVATIVE:\n",
    "# We use numerical differentiation [f(x+δ) - f(x)]/δ for simplicity and to\n",
    "# match what the optimizer actually \"sees\". Analytic form above is for intuition.\n",
    "#\n",
    "# EFFICIENCY ZONE THRESHOLDS:\n",
    "#   mROI > 1.5: EFFICIENT - Every dollar returns >$1.50, strong investment\n",
    "#   mROI 0.8-1.5: DIMINISHING - Still positive but flattening\n",
    "#   mROI < 0.8: SATURATED - Likely better to reallocate to other channels\n",
    "#\n",
    "# These thresholds are heuristics, not physical laws. Adjust based on your\n",
    "# cost of capital and strategic priorities.\n",
    "# =============================================================================\n",
    "\n",
    "def generate_response_curves(X_media, channels, params, coefficients, roi_confidence, n_points=100):\n",
    "    \"\"\"\n",
    "    Generate response curves with confidence intervals and efficiency zones.\n",
    "    \n",
    "    Response curves show the spend → revenue relationship for each channel.\n",
    "    Marginal ROI is the slope of this curve at current spend level.\n",
    "    \n",
    "    ENHANCED OUTPUT INCLUDES:\n",
    "    - CI bands: Upper/lower predictions based on bootstrap coefficient variance\n",
    "    - Marginal ROI at each point: Answers \"what's the next dollar worth HERE?\"\n",
    "    - Efficiency zone: EFFICIENT (mROI > 1.5), DIMINISHING (0.8-1.5), SATURATED (< 0.8)\n",
    "    \"\"\"\n",
    "    curves = []\n",
    "    marginal_roi = {}\n",
    "    \n",
    "    # Re-fit to get current coefficients\n",
    "    X_media_trans = apply_media_transformations(X_media, params, channels)\n",
    "    X_full = pd.concat([X_media_trans, X_control], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_full)\n",
    "    model.fit(X_scaled, y)\n",
    "    coefficients = dict(zip(channels, model.coef_[:len(channels)] / scaler.scale_[:len(channels)]))\n",
    "    \n",
    "    # Get coefficient uncertainty from bootstrap (for CI bands)\n",
    "    coef_std = {}\n",
    "    for ch in channels:\n",
    "        if roi_confidence is not None and 'COEF_STD' in roi_confidence.columns:\n",
    "            ch_row = roi_confidence[roi_confidence['CHANNEL_KEY'] == ch]\n",
    "            coef_std[ch] = ch_row['COEF_STD'].values[0] if len(ch_row) > 0 else 0\n",
    "        else:\n",
    "            coef_std[ch] = coefficients.get(ch, 0) * 0.15  # Default 15% uncertainty\n",
    "    \n",
    "    for ch in channels:\n",
    "        p = params[ch]\n",
    "        coef = coefficients.get(ch, 0)\n",
    "        coef_uncertainty = coef_std.get(ch, 0)\n",
    "        \n",
    "        current_spend = X_media[ch].mean()\n",
    "        max_spend = X_media[ch].max() * 3\n",
    "        gamma = p['gamma']  # Half-saturation point\n",
    "        \n",
    "        spend_range = np.linspace(0, max_spend, n_points)\n",
    "        \n",
    "        for i, spend in enumerate(spend_range):\n",
    "            adstock_steady = spend / (1 - p['theta']) if p['theta'] < 1 else spend\n",
    "            saturated = hill_saturation(np.array([adstock_steady]), p['alpha'], p['gamma'])[0]\n",
    "            contribution = saturated * coef\n",
    "            \n",
    "            # CI bands (scale coefficient uncertainty through saturation transform)\n",
    "            contribution_ci_lower = saturated * max(0, coef - 1.645 * coef_uncertainty)\n",
    "            contribution_ci_upper = saturated * (coef + 1.645 * coef_uncertainty)\n",
    "            \n",
    "            # Marginal ROI at this spend level (numerical derivative)\n",
    "            delta = max(spend * 0.01, 100)  # At least $100 increment\n",
    "            adstock_delta = (spend + delta) / (1 - p['theta']) if p['theta'] < 1 else (spend + delta)\n",
    "            response_delta = hill_saturation(np.array([adstock_delta]), p['alpha'], p['gamma'])[0] * coef\n",
    "            marginal_roi_at_spend = (response_delta - contribution) / delta if delta > 0 else 0\n",
    "            \n",
    "            # Classify efficiency zone based on marginal ROI\n",
    "            if marginal_roi_at_spend > 1.5:\n",
    "                zone = 'EFFICIENT'\n",
    "            elif marginal_roi_at_spend >= 0.8:\n",
    "                zone = 'DIMINISHING'\n",
    "            else:\n",
    "                zone = 'SATURATED'\n",
    "            \n",
    "            curves.append({\n",
    "                'CHANNEL': ch,\n",
    "                'SPEND': spend,\n",
    "                'PREDICTED_REVENUE': contribution,\n",
    "                'PREDICTED_REVENUE_CI_LOWER': contribution_ci_lower,\n",
    "                'PREDICTED_REVENUE_CI_UPPER': contribution_ci_upper,\n",
    "                'MARGINAL_ROI_AT_SPEND': marginal_roi_at_spend,\n",
    "                'EFFICIENCY_ZONE': zone\n",
    "            })\n",
    "        \n",
    "        # Marginal ROI at current spend (for summary)\n",
    "        delta = current_spend * 0.01\n",
    "        adstock_curr = current_spend / (1 - p['theta']) if p['theta'] < 1 else current_spend\n",
    "        response_curr = hill_saturation(np.array([adstock_curr]), p['alpha'], p['gamma'])[0] * coef\n",
    "        \n",
    "        adstock_delta = (current_spend + delta) / (1 - p['theta']) if p['theta'] < 1 else (current_spend + delta)\n",
    "        response_delta = hill_saturation(np.array([adstock_delta]), p['alpha'], p['gamma'])[0] * coef\n",
    "        \n",
    "        marginal_roi[ch] = (response_delta - response_curr) / delta if delta > 0 else 0\n",
    "    \n",
    "    return pd.DataFrame(curves), marginal_roi\n",
    "\n",
    "# Generate curves with CI bands\n",
    "response_curves, marginal_roi = generate_response_curves(X_media, channels, best_params, {}, roi_confidence)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MARGINAL ROI (Value of Next Dollar Spent)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTop 10 Channels by Marginal ROI:\")\n",
    "marginal_df = pd.DataFrame([\n",
    "    {'CHANNEL_KEY': ch, 'MARGINAL_ROI': roi} \n",
    "    for ch, roi in marginal_roi.items()\n",
    "]).sort_values('MARGINAL_ROI', ascending=False)\n",
    "print(marginal_df.head(10).to_string(index=False))\n",
    "print(f\"\\nResponse curves generated: {len(response_curves)} data points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "optimize_budget_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Budget Optimizer\n",
    "# =============================================================================\n",
    "#\n",
    "# THE BUDGET ALLOCATION PROBLEM:\n",
    "#\n",
    "# Given the learned response curves, how should we reallocate spend to maximize\n",
    "# total revenue? This is a constrained optimization problem:\n",
    "#\n",
    "#   MAXIMIZE: Total predicted revenue = Σ (saturated_response × coefficient)\n",
    "#   SUBJECT TO:\n",
    "#     1. Total budget unchanged (budget neutral)\n",
    "#     2. Each channel can only change ±30% (realistic for CMO approval)\n",
    "#     3. All spend >= 0 (can't have negative spend)\n",
    "#\n",
    "# WHY CONSTRAINTS MATTER:\n",
    "#\n",
    "# Without constraints, the optimizer would say \"put 100% in LinkedIn\" because\n",
    "# it has the highest marginal ROI. But this is impractical:\n",
    "#   - CMOs can't pivot all spend in one quarter\n",
    "#   - Vendor contracts require minimum commitments\n",
    "#   - Channel capacity limits exist (LinkedIn inventory is finite)\n",
    "#\n",
    "# The ±30% limit keeps recommendations actionable. If LinkedIn is at $1M/quarter,\n",
    "# we recommend up to $1.3M, not $10M.\n",
    "#\n",
    "# SLSQP ALGORITHM:\n",
    "# Sequential Least Squares Programming - a constrained optimizer that handles\n",
    "# both equality constraints (total budget) and inequality constraints (±30%).\n",
    "# Faster than evolutionary methods for smooth, convex problems like this.\n",
    "#\n",
    "# PREDICTED LIFT:\n",
    "# The \"predicted lift\" is the difference between:\n",
    "#   - Current revenue (with current allocation)\n",
    "#   - Optimized revenue (with recommended allocation)\n",
    "# This is the \"headline number\" for the CMO: \"Shifting spend could add $2.4M\"\n",
    "#\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEEPER DIVE: CONSTRAINED OPTIMIZATION FORMULATION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# We work in PROPORTION space (x_i = spend_i / total_budget) for numerical\n",
    "# stability and cleaner constraint expression.\n",
    "#\n",
    "# FORMAL PROBLEM:\n",
    "#   min  -Σ f_i(x_i · B)      (we negate because scipy.minimize minimizes)\n",
    "#   s.t. Σ x_i = 1            (equality: budget-neutral)\n",
    "#        (1-δ)·x_i^0 ≤ x_i ≤ (1+δ)·x_i^0  for all i  (inequality: ±δ bounds)\n",
    "#        x_i ≥ 0              for all i  (implicit in bounds)\n",
    "#\n",
    "# where:\n",
    "#   f_i(s) = β_i · H_i(A_i(s))  is the response function for channel i\n",
    "#   B = total budget\n",
    "#   x_i^0 = current proportion for channel i\n",
    "#   δ = 0.30 (our 30% change limit)\n",
    "#\n",
    "# WHY SLSQP (Sequential Least Squares Quadratic Programming):\n",
    "# 1. Handles both equality (budget) and inequality (bounds) constraints\n",
    "# 2. Uses quadratic approximation of the Lagrangian at each step\n",
    "# 3. Convergence is typically fast for smooth, moderately-sized problems\n",
    "# 4. Available in scipy.optimize.minimize with method='SLSQP'\n",
    "#\n",
    "# The Lagrangian for our problem:\n",
    "#   L(x, λ, μ) = -Σ f_i(x_i·B) + λ·(Σx_i - 1) + Σ μ_i·(constraint violations)\n",
    "#\n",
    "# At the optimum, KKT conditions require:\n",
    "#   ∂f_i/∂x_i = λ for all active channels (marginal returns equalized!)\n",
    "#\n",
    "# ECONOMIC INTERPRETATION:\n",
    "# The optimal allocation EQUALIZES MARGINAL ROI across all channels (subject\n",
    "# to constraints). This is the principle of marginal analysis: reallocate from\n",
    "# low-mROI to high-mROI channels until they're equal.\n",
    "#\n",
    "# If channel A has mROI = 3.0 and channel B has mROI = 1.5, we should shift\n",
    "# budget A→B until they converge (typically around 2.0 for both).\n",
    "#\n",
    "# The ±30% constraint prevents extreme shifts, so post-optimization mROIs\n",
    "# won't be perfectly equal—but they'll be closer than the starting point.\n",
    "#\n",
    "# CAUTION ON \"PREDICTED LIFT\":\n",
    "# The lift estimate assumes the model is correctly specified and extrapolates\n",
    "# well. In practice, treat it as directional. A \"10% lift\" doesn't mean you'll\n",
    "# get exactly 10%—it means reallocation is likely beneficial.\n",
    "# =============================================================================\n",
    "\n",
    "def optimize_budget(X_media, channels, params, marginal_roi, budget_change_limit=0.30):\n",
    "    \"\"\"\n",
    "    Optimize budget allocation to maximize predicted revenue.\n",
    "    \n",
    "    Uses constrained optimization (SLSQP) to find the best reallocation\n",
    "    within business constraints (±30% per channel, budget neutral).\n",
    "    \"\"\"\n",
    "    current_spend = {ch: X_media[ch].sum() for ch in channels}\n",
    "    total_budget = sum(current_spend.values())\n",
    "    \n",
    "    # Only optimize channels with actual spend (avoid divide-by-zero)\n",
    "    active_channels = [ch for ch in channels if current_spend[ch] > 0]\n",
    "    n_channels = len(active_channels)\n",
    "    \n",
    "    if n_channels == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Re-fit model to get coefficients (needed for revenue prediction)\n",
    "    X_media_trans = apply_media_transformations(X_media, params, channels)\n",
    "    X_full = pd.concat([X_media_trans, X_control], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_full)\n",
    "    model.fit(X_scaled, y)\n",
    "    # Unscale coefficients to get \"per-unit\" impact\n",
    "    coefficients = dict(zip(channels, model.coef_[:len(channels)] / scaler.scale_[:len(channels)]))\n",
    "    \n",
    "    x0 = np.array([current_spend[ch] / total_budget for ch in active_channels])\n",
    "    \n",
    "    def objective(x):\n",
    "        total_contrib = 0\n",
    "        for i, ch in enumerate(active_channels):\n",
    "            spend = x[i] * total_budget\n",
    "            p = params[ch]\n",
    "            coef = coefficients.get(ch, 0)\n",
    "            \n",
    "            weekly_spend = spend / len(X_media)\n",
    "            adstock = weekly_spend / (1 - p['theta']) if p['theta'] < 1 else weekly_spend\n",
    "            saturated = hill_saturation(np.array([adstock]), p['alpha'], p['gamma'])[0]\n",
    "            contribution = saturated * coef * len(X_media)\n",
    "            total_contrib += contribution\n",
    "        \n",
    "        return -total_contrib\n",
    "    \n",
    "    budget_constraint = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1.0}\n",
    "    \n",
    "    bounds = []\n",
    "    for i, ch in enumerate(active_channels):\n",
    "        current_prop = x0[i]\n",
    "        lower = max(0, current_prop * (1 - budget_change_limit))\n",
    "        upper = current_prop * (1 + budget_change_limit)\n",
    "        bounds.append((lower, upper))\n",
    "    \n",
    "    result = minimize(objective, x0, method='SLSQP', bounds=bounds, \n",
    "                     constraints=budget_constraint, options={'maxiter': 1000})\n",
    "    \n",
    "    results = []\n",
    "    for i, ch in enumerate(active_channels):\n",
    "        current = current_spend[ch]\n",
    "        recommended = result.x[i] * total_budget\n",
    "        change = recommended - current\n",
    "        change_pct = change / current * 100 if current > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'CHANNEL_KEY': ch,\n",
    "            'CURRENT_SPEND': current,\n",
    "            'RECOMMENDED_SPEND': recommended,\n",
    "            'CHANGE_AMOUNT': change,\n",
    "            'CHANGE_PCT': change_pct,\n",
    "            'MARGINAL_ROI': marginal_roi.get(ch, 0)\n",
    "        })\n",
    "    \n",
    "    opt_df = pd.DataFrame(results).sort_values('CHANGE_PCT', ascending=False)\n",
    "    \n",
    "    current_contribution = -objective(x0)\n",
    "    optimized_contribution = -objective(result.x)\n",
    "    predicted_lift = optimized_contribution - current_contribution\n",
    "    \n",
    "    print(f\"\\nPredicted Revenue Lift: ${predicted_lift:,.0f}\")\n",
    "    print(f\"Lift Percentage: {predicted_lift / current_contribution * 100:.1f}%\")\n",
    "    \n",
    "    return opt_df\n",
    "\n",
    "# Run budget optimization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUDGET OPTIMIZATION RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConstraints: Budget neutral, ±{config.budget_change_limit*100:.0f}% per channel\")\n",
    "\n",
    "budget_recommendations = optimize_budget(\n",
    "    X_media, channels, best_params, marginal_roi, config.budget_change_limit\n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 Channels to INCREASE:\")\n",
    "print(budget_recommendations.head(5)[['CHANNEL_KEY', 'CURRENT_SPEND', 'RECOMMENDED_SPEND', 'CHANGE_PCT']].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 5 Channels to DECREASE:\")\n",
    "print(budget_recommendations.tail(5)[['CHANNEL_KEY', 'CURRENT_SPEND', 'RECOMMENDED_SPEND', 'CHANGE_PCT']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "prepare_model_results_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Prepare Results with Dimensional Keys\n",
    "# =============================================================================\n",
    "#\n",
    "# OUTPUT STRUCTURE:\n",
    "#\n",
    "# The notebook produces granular ROI at the Channel × Region × Product level.\n",
    "# Example: \"LINKEDIN_EMEA_SI\" → LinkedIn in EMEA for Safety & Industrial.\n",
    "#\n",
    "# We parse this back to separate columns so results can JOIN to dimension tables:\n",
    "#   - CHANNEL_CODE → joins to MARKETING_CHANNEL dimension\n",
    "#   - GEO_CODE → joins to GEOGRAPHY dimension (at configured level)\n",
    "#   - PRODUCT_CODE → joins to PRODUCT_CATEGORY dimension (at configured level)\n",
    "#\n",
    "# This enables the Streamlit app to filter/slice results by any dimension\n",
    "# without string parsing in SQL.\n",
    "#\n",
    "# KEY OUTPUT COLUMNS:\n",
    "#   - ROI: Average historical return (contribution / spend)\n",
    "#   - ROI_CI_LOWER/UPPER: Bootstrap confidence bounds\n",
    "#   - MARGINAL_ROI: Return on NEXT dollar (derivative of response curve)\n",
    "#   - OPTIMAL_SPEND_SUGGESTION: Budget optimizer recommendation\n",
    "#   - ADSTOCK_DECAY_RATE: Learned theta (how quickly effect fades)\n",
    "#   - SATURATION_POINT: Learned gamma (spend level at 50% of max response)\n",
    "# =============================================================================\n",
    "\n",
    "def parse_channel_key(channel_key):\n",
    "    \"\"\"\n",
    "    Parse composite channel key back to dimensions.\n",
    "    E.g., \"LINKEDIN_EMEA_SI\" → {CHANNEL: LINKEDIN, GEO: EMEA, PRODUCT: SI}\n",
    "    \"\"\"\n",
    "    parts = channel_key.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        return {'CHANNEL': parts[0], 'GEO': parts[1], 'PRODUCT': parts[2]}\n",
    "    elif len(parts) == 2:\n",
    "        return {'CHANNEL': parts[0], 'GEO': parts[1], 'PRODUCT': 'ALL'}\n",
    "    else:\n",
    "        return {'CHANNEL': parts[0] if parts else 'UNKNOWN', 'GEO': 'ALL', 'PRODUCT': 'ALL'}\n",
    "\n",
    "\n",
    "def prepare_model_results(roi_confidence, marginal_roi, budget_recommendations, params, config, metrics, X_media):\n",
    "    \"\"\"\n",
    "    Prepare final results DataFrame for saving to MMM.MODEL_RESULTS.\n",
    "    \n",
    "    ENHANCED OUTPUT now includes:\n",
    "    - Full uncertainty quantification (CI bounds, significance)\n",
    "    - Learned MMM parameters (adstock decay, saturation shape/scale)\n",
    "    - Model quality metrics (R², CV MAPE)\n",
    "    - Spend context (current spend, share of budget)\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    ci_level = int(config.confidence_level * 100)\n",
    "    \n",
    "    # Calculate total spend across all channels for share calculation\n",
    "    total_spend_all = sum(row['TOTAL_SPEND'] for _, row in roi_confidence.iterrows())\n",
    "    \n",
    "    for _, row in roi_confidence.iterrows():\n",
    "        ch = row['CHANNEL_KEY']\n",
    "        dims = parse_channel_key(ch)\n",
    "        p = params.get(ch, {'theta': 0, 'alpha': 1, 'gamma': 1})\n",
    "        \n",
    "        budget_row = budget_recommendations[budget_recommendations['CHANNEL_KEY'] == ch]\n",
    "        optimal_spend = budget_row['RECOMMENDED_SPEND'].values[0] if len(budget_row) > 0 else row['TOTAL_SPEND']\n",
    "        \n",
    "        # Calculate spend share\n",
    "        spend_share = row['TOTAL_SPEND'] / total_spend_all if total_spend_all > 0 else 0\n",
    "        \n",
    "        # Count observations for this channel\n",
    "        n_obs = len(X_media[ch].dropna()) if ch in X_media.columns else 0\n",
    "        \n",
    "        results.append({\n",
    "            # Identifiers\n",
    "            'MODEL_RUN_DATE': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'MODEL_VERSION': config.model_version,\n",
    "            'CHANNEL_CODE': dims['CHANNEL'],\n",
    "            'GEO_CODE': dims['GEO'],\n",
    "            'PRODUCT_CODE': dims['PRODUCT'],\n",
    "            'CHANNEL_KEY': ch,\n",
    "            \n",
    "            # Core metrics\n",
    "            'COEFFICIENT_WEIGHT': row['COEF_MEAN'],\n",
    "            'ROI': row['ROI_MEAN'],\n",
    "            'MARGINAL_ROI': marginal_roi.get(ch, 0),\n",
    "            \n",
    "            # Confidence intervals (90% CI from bootstrap)\n",
    "            'ROI_CI_LOWER': row[f'ROI_CI_LOWER_{ci_level}'],\n",
    "            'ROI_CI_UPPER': row[f'ROI_CI_UPPER_{ci_level}'],\n",
    "            'IS_SIGNIFICANT': row['IS_SIGNIFICANT'],\n",
    "            \n",
    "            # Learned MMM parameters\n",
    "            'ADSTOCK_DECAY_RATE': p['theta'],\n",
    "            'SATURATION_ALPHA': p['alpha'],  # Hill shape parameter\n",
    "            'SATURATION_POINT': p['gamma'],   # Half-saturation spend level\n",
    "            \n",
    "            # Model quality\n",
    "            'MODEL_R2_INSAMPLE': metrics['in_sample']['R2'],\n",
    "            'MODEL_MAPE_CV': metrics['cv_mean'].get('MAPE', None),\n",
    "            'N_OBSERVATIONS': n_obs,\n",
    "            \n",
    "            # Spend context\n",
    "            'CURRENT_SPEND': row['TOTAL_SPEND'],\n",
    "            'SPEND_SHARE': spend_share,\n",
    "            'OPTIMAL_SPEND_SUGGESTION': optimal_spend\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Prepare results with enhanced fields\n",
    "model_results = prepare_model_results(\n",
    "    roi_confidence, marginal_roi, budget_recommendations, best_params, config, metrics, X_media\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal records: {len(model_results)}\")\n",
    "print(f\"Model version: {config.model_version}\")\n",
    "print(f\"\\nColumns: {model_results.columns.tolist()}\")\n",
    "model_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "save_to_snowflake_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Save Results to Snowflake\n",
    "# =============================================================================\n",
    "#\n",
    "# THREE OUTPUT TABLES (all overwrite mode for idempotent execution):\n",
    "#\n",
    "# 1. MMM.MODEL_RESULTS (overwrite mode)\n",
    "#    - Channel-level results with ROI, confidence intervals, and parameters\n",
    "#    - Used by: Streamlit app, analysis views\n",
    "#\n",
    "# 2. MMM.RESPONSE_CURVES (overwrite mode)\n",
    "#    - Detailed spend → revenue curves for visualization\n",
    "#    - 100 points per channel (0 to 3x max spend)\n",
    "#    - Used by: Streamlit \"What-If Simulator\" charts\n",
    "#\n",
    "# 3. MMM.MODEL_METADATA (overwrite mode)\n",
    "#    - Model configuration and quality metrics\n",
    "#    - Tracks: R², MAPE, hyperparameter settings\n",
    "#    - Used for: Model quality monitoring\n",
    "#\n",
    "# All tables use OVERWRITE to ensure clean, idempotent results on each run.\n",
    "# Historical tracking can be done via a separate versioning/archival process.\n",
    "# =============================================================================\n",
    "\n",
    "def save_to_snowflake(session, model_results, response_curves, config, metrics):\n",
    "    \"\"\"\n",
    "    Save model results and response curves to Snowflake.\n",
    "    \n",
    "    OUTPUT TABLES:\n",
    "    - MMM.MODEL_RESULTS: Channel-level ROI with confidence intervals and parameters\n",
    "    - MMM.RESPONSE_CURVES: Detailed curves with CI bands and efficiency zones\n",
    "    - MMM.MODEL_METADATA: Model configuration and quality metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nSaving results to Snowflake...\")\n",
    "    \n",
    "    # 1. Save main results to MMM.MODEL_RESULTS (enhanced schema)\n",
    "    results_clean = model_results.copy()\n",
    "    \n",
    "    # Map DataFrame columns to table columns\n",
    "    results_for_db = pd.DataFrame({\n",
    "        'MODEL_VERSION': results_clean['MODEL_VERSION'],\n",
    "        'CHANNEL': results_clean['CHANNEL_KEY'],\n",
    "        'COEFF_WEIGHT': results_clean['COEFFICIENT_WEIGHT'],\n",
    "        'ROI': results_clean['ROI'],\n",
    "        'MARGINAL_ROI': results_clean['MARGINAL_ROI'],\n",
    "        'OPTIMAL_SPEND': results_clean['OPTIMAL_SPEND_SUGGESTION'],\n",
    "        # Confidence intervals\n",
    "        'ROI_CI_LOWER': results_clean['ROI_CI_LOWER'],\n",
    "        'ROI_CI_UPPER': results_clean['ROI_CI_UPPER'],\n",
    "        'IS_SIGNIFICANT': results_clean['IS_SIGNIFICANT'],\n",
    "        # Learned parameters\n",
    "        'ADSTOCK_DECAY': results_clean['ADSTOCK_DECAY_RATE'],\n",
    "        'SATURATION_ALPHA': results_clean['SATURATION_ALPHA'],\n",
    "        'SATURATION_GAMMA': results_clean['SATURATION_POINT'],\n",
    "        # Model quality\n",
    "        'CV_MAPE': results_clean['MODEL_MAPE_CV'],\n",
    "        'R_SQUARED': results_clean['MODEL_R2_INSAMPLE'],\n",
    "        'N_OBSERVATIONS': results_clean['N_OBSERVATIONS'],\n",
    "        # Spend context\n",
    "        'CURRENT_SPEND': results_clean['CURRENT_SPEND'],\n",
    "        'SPEND_SHARE': results_clean['SPEND_SHARE']\n",
    "    })\n",
    "    \n",
    "    results_sf = session.create_dataframe(results_for_db)\n",
    "    results_sf.write.mode(\"overwrite\").save_as_table(\"MMM.MODEL_RESULTS\")\n",
    "    print(f\"  ✓ Saved {len(results_for_db)} rows to MMM.MODEL_RESULTS\")\n",
    "    \n",
    "    # Legacy ATOMIC table save removed - schema incompatible with flat output\n",
    "    # Use MMM.MODEL_RESULTS as the primary results table\n",
    "    print(f\"  ✓ Skipping legacy ATOMIC.MMM_MODEL_RESULT (use MMM.MODEL_RESULTS instead)\")\n",
    "    \n",
    "    # 2. Save enhanced response curves (with CI bands and efficiency zones)\n",
    "    curves_clean = response_curves.copy()\n",
    "    curves_clean['MODEL_VERSION'] = config.model_version\n",
    "    \n",
    "    curves_sf = session.create_dataframe(curves_clean)\n",
    "    curves_sf.write.mode(\"overwrite\").save_as_table(\"MMM.RESPONSE_CURVES\")\n",
    "    print(f\"  ✓ Saved {len(curves_clean)} rows to MMM.RESPONSE_CURVES\")\n",
    "    print(f\"    → Includes: CI bands, marginal ROI at each point, efficiency zones\")\n",
    "    \n",
    "    # 3. Save model metadata\n",
    "    metadata = pd.DataFrame([{\n",
    "        'MODEL_VERSION': config.model_version,\n",
    "        'MODEL_RUN_DATE': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'GEO_LEVEL': config.geo_level,\n",
    "        'PRODUCT_LEVEL': config.product_level,\n",
    "        'N_CHANNELS': len(model_results),\n",
    "        'R2_INSAMPLE': metrics['in_sample']['R2'],\n",
    "        'MAPE_CV': metrics['cv_mean'].get('MAPE', None),\n",
    "        'NEVERGRAD_BUDGET': config.nevergrad_budget,\n",
    "        'N_BOOTSTRAP': config.n_bootstrap,\n",
    "        'CONFIDENCE_LEVEL': config.confidence_level\n",
    "    }])\n",
    "    \n",
    "    metadata_sf = session.create_dataframe(metadata)\n",
    "    metadata_sf.write.mode(\"overwrite\").save_as_table(\"MMM.MODEL_METADATA\")\n",
    "    print(f\"  ✓ Saved model metadata to MMM.MODEL_METADATA\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVE COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nEnhanced outputs include:\")\n",
    "    print(f\"  • 90% confidence intervals on all ROI estimates\")\n",
    "    print(f\"  • Learned adstock decay and saturation parameters per channel\")\n",
    "    print(f\"  • Response curve CI bands and efficiency zone classifications\")\n",
    "\n",
    "# Save to Snowflake\n",
    "save_to_snowflake(session, model_results, response_curves, config, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "executive_summary_display"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: Executive Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    MMM TRAINING EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODEL CONFIGURATION\n",
    "-------------------\n",
    "Version:           {config.model_version}\n",
    "Geographic Level:  {config.geo_level}\n",
    "Product Level:     {config.product_level}\n",
    "Channels Modeled:  {len(channels)}\n",
    "\n",
    "MODEL QUALITY\n",
    "-------------\n",
    "In-Sample R²:      {metrics['in_sample']['R2']:.4f}\n",
    "CV MAPE:           {metrics['cv_mean'].get('MAPE', 0):.1f}% ± {metrics['cv_std'].get('MAPE', 0):.1f}%\n",
    "CV R²:             {metrics['cv_mean'].get('R2', 0):.4f} ± {metrics['cv_std'].get('R2', 0):.4f}\n",
    "\n",
    "TOP PERFORMING CHANNELS (by ROI)\n",
    "--------------------------------\"\"\")\n",
    "\n",
    "ci_level = int(config.confidence_level * 100)\n",
    "top_channels = roi_confidence.head(5)\n",
    "for _, row in top_channels.iterrows():\n",
    "    sig = \"*\" if row['IS_SIGNIFICANT'] else \"\"\n",
    "    print(f\"  {row['CHANNEL_KEY']}: ROI = {row['ROI_MEAN']:.2f} [{row[f'ROI_CI_LOWER_{ci_level}']:.2f}, {row[f'ROI_CI_UPPER_{ci_level}']:.2f}] {sig}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "BUDGET OPTIMIZATION HIGHLIGHTS\n",
    "------------------------------\n",
    "Constraint: ±{config.budget_change_limit*100:.0f}% per channel (budget neutral)\"\"\")\n",
    "\n",
    "if len(budget_recommendations) > 0:\n",
    "    top_increase = budget_recommendations.head(3)\n",
    "    top_decrease = budget_recommendations.tail(3)\n",
    "    \n",
    "    print(\"\\nIncrease Spend:\")\n",
    "    for _, row in top_increase.iterrows():\n",
    "        print(f\"  {row['CHANNEL_KEY']}: +{row['CHANGE_PCT']:.1f}% (${row['CHANGE_AMOUNT']:,.0f})\")\n",
    "    \n",
    "    print(\"\\nDecrease Spend:\")\n",
    "    for _, row in top_decrease.iterrows():\n",
    "        print(f\"  {row['CHANNEL_KEY']}: {row['CHANGE_PCT']:.1f}% (${row['CHANGE_AMOUNT']:,.0f})\")\n",
    "\n",
    "print(f\"\"\"\n",
    "OUTPUT TABLES\n",
    "-------------\n",
    "• ATOMIC.MMM_MODEL_RESULT    - Channel results with dimensional keys\n",
    "• MMM.RESPONSE_CURVES        - Spend vs. revenue curves\n",
    "• MMM.MODEL_METADATA         - Model configuration and quality\n",
    "\n",
    "NEXT STEPS\n",
    "----------\n",
    "1. Review results in DIMENSIONAL.V_MMM_RESULTS_ANALYSIS view\n",
    "2. Visualize response curves in Streamlit app\n",
    "3. Use Budget Optimizer for scenario planning\n",
    "4. Monitor model drift and retrain quarterly\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Model training completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
